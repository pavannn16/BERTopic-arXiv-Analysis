{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6429f7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "%pip install bertopic sentence-transformers umap-learn hdbscan gensim plotly scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a448e13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# PROJECT PATH SETUP\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    PROJECT_PATH = '/content/drive/MyDrive/BERTopic-arXiv-Analysis'\n",
    "    print(\"âœ… Running on Google Colab\")\n",
    "else:\n",
    "    PROJECT_PATH = str(Path(os.getcwd()).parent) if 'notebooks' in os.getcwd() else os.getcwd()\n",
    "    print(\"âœ… Running locally\")\n",
    "\n",
    "print(f\"ðŸ“ Project path: {PROJECT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5545f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# BERTopic components\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Evaluation\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora import Dictionary\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "print(\"Libraries imported!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdbe62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(f\"âœ… GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print(\"âœ… Apple Silicon GPU available\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"âš ï¸ No GPU - using CPU (will be slower)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a9190e",
   "metadata": {},
   "source": [
    "## 1. Load Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4c6639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "df = pd.read_csv(f\"{PROJECT_PATH}/data/processed/arxiv_cs_ai_processed.csv\")\n",
    "documents = df['text'].tolist()\n",
    "\n",
    "print(f\"Loaded {len(documents)} documents\")\n",
    "print(f\"Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "\n",
    "# Prepare tokenized docs for coherence computation\n",
    "tokenized_docs = [doc.lower().split() for doc in documents]\n",
    "dictionary = Dictionary(tokenized_docs)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.95)\n",
    "print(f\"Dictionary size: {len(dictionary)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a075f668",
   "metadata": {},
   "source": [
    "## 2. Embedding Model Comparison\n",
    "\n",
    "We compare two popular Sentence-BERT models:\n",
    "\n",
    "| Model | Parameters | Speed | Quality |\n",
    "|-------|------------|-------|--------|\n",
    "| **all-mpnet-base-v2** | 110M | Slower | Higher |\n",
    "| **all-MiniLM-L6-v2** | 22M | 5x faster | Good |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da2c7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define embedding models to compare\n",
    "EMBEDDING_MODELS = {\n",
    "    'mpnet': 'all-mpnet-base-v2',\n",
    "    'minilm': 'all-MiniLM-L6-v2'\n",
    "}\n",
    "\n",
    "embeddings_dict = {}\n",
    "\n",
    "for name, model_name in EMBEDDING_MODELS.items():\n",
    "    embeddings_path = f\"{PROJECT_PATH}/data/embeddings/embeddings_{name}.npy\"\n",
    "    \n",
    "    if os.path.exists(embeddings_path):\n",
    "        print(f\"Loading pre-computed {name} embeddings...\")\n",
    "        embeddings_dict[name] = np.load(embeddings_path)\n",
    "    else:\n",
    "        print(f\"\\nComputing embeddings with {model_name}...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        model = SentenceTransformer(model_name)\n",
    "        embeddings = model.encode(\n",
    "            documents,\n",
    "            batch_size=64,\n",
    "            show_progress_bar=True,\n",
    "            convert_to_numpy=True,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"  Time: {elapsed:.1f}s ({len(documents)/elapsed:.1f} docs/sec)\")\n",
    "        \n",
    "        # Save embeddings\n",
    "        np.save(embeddings_path, embeddings)\n",
    "        embeddings_dict[name] = embeddings\n",
    "        print(f\"  Saved to {embeddings_path}\")\n",
    "\n",
    "print(f\"\\nâœ… Embeddings ready:\")\n",
    "for name, emb in embeddings_dict.items():\n",
    "    print(f\"  {name}: {emb.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596d597c",
   "metadata": {},
   "source": [
    "## 3. Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4660a025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(topic_model, documents, embeddings, topics, tokenized_docs, dictionary):\n",
    "    \"\"\"\n",
    "    Compute all evaluation metrics for a BERTopic model.\n",
    "    \n",
    "    Returns:\n",
    "        dict with coherence, diversity, silhouette, n_topics, outlier_pct\n",
    "    \"\"\"\n",
    "    # Get topic words\n",
    "    topics_dict = topic_model.get_topics()\n",
    "    if -1 in topics_dict:\n",
    "        del topics_dict[-1]\n",
    "    \n",
    "    n_topics = len(topics_dict)\n",
    "    \n",
    "    # Outlier percentage\n",
    "    n_outliers = sum(1 for t in topics if t == -1)\n",
    "    outlier_pct = 100 * n_outliers / len(topics)\n",
    "    \n",
    "    # Topic Diversity\n",
    "    all_words = []\n",
    "    topic_words_list = []\n",
    "    for topic_id in sorted(topics_dict.keys()):\n",
    "        words = [word for word, _ in topics_dict[topic_id][:10]]\n",
    "        topic_words_list.append(words)\n",
    "        all_words.extend(words)\n",
    "    \n",
    "    diversity = len(set(all_words)) / len(all_words) if all_words else 0\n",
    "    \n",
    "    # Coherence (NPMI)\n",
    "    try:\n",
    "        coherence_model = CoherenceModel(\n",
    "            topics=topic_words_list,\n",
    "            texts=tokenized_docs,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_npmi'\n",
    "        )\n",
    "        coherence = coherence_model.get_coherence()\n",
    "    except:\n",
    "        coherence = 0.0\n",
    "    \n",
    "    # Silhouette Score\n",
    "    try:\n",
    "        mask = np.array(topics) != -1\n",
    "        if mask.sum() > 100 and len(set(np.array(topics)[mask])) > 1:\n",
    "            silhouette = silhouette_score(embeddings[mask], np.array(topics)[mask])\n",
    "        else:\n",
    "            silhouette = 0.0\n",
    "    except:\n",
    "        silhouette = 0.0\n",
    "    \n",
    "    return {\n",
    "        'n_topics': n_topics,\n",
    "        'outlier_pct': outlier_pct,\n",
    "        'coherence': coherence,\n",
    "        'diversity': diversity,\n",
    "        'silhouette': silhouette,\n",
    "        'combined_score': coherence * 0.5 + diversity * 0.3 + (1 - outlier_pct/100) * 0.2\n",
    "    }\n",
    "\n",
    "print(\"Evaluation functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f402abd4",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Grid Search\n",
    "\n",
    "We'll search over:\n",
    "- **Embedding model**: MPNet vs MiniLM\n",
    "- **min_cluster_size**: [10, 15, 20, 30, 50]\n",
    "- **n_neighbors**: [10, 15, 25]\n",
    "- **n_components**: [5, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa42b6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "PARAM_GRID = {\n",
    "    'embedding_model': ['mpnet', 'minilm'],\n",
    "    'min_cluster_size': [10, 15, 20, 30, 50],\n",
    "    'n_neighbors': [10, 15, 25],\n",
    "    'n_components': [5, 10]\n",
    "}\n",
    "\n",
    "# Calculate total combinations\n",
    "n_combinations = 1\n",
    "for values in PARAM_GRID.values():\n",
    "    n_combinations *= len(values)\n",
    "\n",
    "print(f\"Total hyperparameter combinations: {n_combinations}\")\n",
    "print(f\"Estimated time: {n_combinations * 0.5:.0f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a5b712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run grid search\n",
    "results = []\n",
    "best_score = -float('inf')\n",
    "best_params = None\n",
    "best_model = None\n",
    "\n",
    "print(\"Starting hyperparameter search...\\n\")\n",
    "\n",
    "for emb_name in PARAM_GRID['embedding_model']:\n",
    "    embeddings = embeddings_dict[emb_name]\n",
    "    \n",
    "    for min_cluster_size in PARAM_GRID['min_cluster_size']:\n",
    "        for n_neighbors in PARAM_GRID['n_neighbors']:\n",
    "            for n_components in PARAM_GRID['n_components']:\n",
    "                \n",
    "                params = {\n",
    "                    'embedding_model': emb_name,\n",
    "                    'min_cluster_size': min_cluster_size,\n",
    "                    'n_neighbors': n_neighbors,\n",
    "                    'n_components': n_components\n",
    "                }\n",
    "                \n",
    "                try:\n",
    "                    # Build model\n",
    "                    umap_model = UMAP(\n",
    "                        n_neighbors=n_neighbors,\n",
    "                        n_components=n_components,\n",
    "                        min_dist=0.0,\n",
    "                        metric='cosine',\n",
    "                        random_state=42\n",
    "                    )\n",
    "                    \n",
    "                    hdbscan_model = HDBSCAN(\n",
    "                        min_cluster_size=min_cluster_size,\n",
    "                        min_samples=10,\n",
    "                        metric='euclidean',\n",
    "                        cluster_selection_method='eom',\n",
    "                        prediction_data=True\n",
    "                    )\n",
    "                    \n",
    "                    vectorizer_model = CountVectorizer(\n",
    "                        ngram_range=(1, 2),\n",
    "                        stop_words='english',\n",
    "                        min_df=5,\n",
    "                        max_df=0.95\n",
    "                    )\n",
    "                    \n",
    "                    topic_model = BERTopic(\n",
    "                        umap_model=umap_model,\n",
    "                        hdbscan_model=hdbscan_model,\n",
    "                        vectorizer_model=vectorizer_model,\n",
    "                        top_n_words=10,\n",
    "                        calculate_probabilities=False,\n",
    "                        verbose=False\n",
    "                    )\n",
    "                    \n",
    "                    # Fit model\n",
    "                    topics, _ = topic_model.fit_transform(documents, embeddings=embeddings)\n",
    "                    \n",
    "                    # Compute metrics\n",
    "                    metrics = compute_metrics(\n",
    "                        topic_model, documents, embeddings, topics,\n",
    "                        tokenized_docs, dictionary\n",
    "                    )\n",
    "                    metrics.update(params)\n",
    "                    results.append(metrics)\n",
    "                    \n",
    "                    # Check if best\n",
    "                    if metrics['combined_score'] > best_score:\n",
    "                        best_score = metrics['combined_score']\n",
    "                        best_params = params.copy()\n",
    "                        best_model = topic_model\n",
    "                        best_topics = topics\n",
    "                        best_embeddings = embeddings\n",
    "                    \n",
    "                    print(f\"âœ“ {emb_name}, mcs={min_cluster_size}, nn={n_neighbors}, nc={n_components} â†’ \"\n",
    "                          f\"Topics: {metrics['n_topics']}, Coh: {metrics['coherence']:.3f}, \"\n",
    "                          f\"Div: {metrics['diversity']:.3f}, Out: {metrics['outlier_pct']:.1f}%\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"âœ— {emb_name}, mcs={min_cluster_size}, nn={n_neighbors}, nc={n_components} â†’ Error: {str(e)[:50]}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"GRID SEARCH COMPLETE!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0f22d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('combined_score', ascending=False)\n",
    "\n",
    "print(\"Top 10 Configurations:\")\n",
    "display_cols = ['embedding_model', 'min_cluster_size', 'n_neighbors', 'n_components',\n",
    "                'n_topics', 'coherence', 'diversity', 'outlier_pct', 'combined_score']\n",
    "print(results_df[display_cols].head(10).to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(f\"{PROJECT_PATH}/results/hyperparameter_search_results.csv\", index=False)\n",
    "print(f\"\\nResults saved to hyperparameter_search_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2771013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize hyperparameter effects\n",
    "fig = make_subplots(rows=2, cols=2, subplot_titles=[\n",
    "    'Coherence by min_cluster_size',\n",
    "    'Coherence by Embedding Model',\n",
    "    'Topics vs Coherence',\n",
    "    'Outlier % vs Coherence'\n",
    "])\n",
    "\n",
    "# 1. min_cluster_size effect\n",
    "for emb in results_df['embedding_model'].unique():\n",
    "    subset = results_df[results_df['embedding_model'] == emb]\n",
    "    grouped = subset.groupby('min_cluster_size')['coherence'].mean()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=grouped.index, y=grouped.values, name=emb, mode='lines+markers'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "# 2. Embedding model comparison\n",
    "emb_comparison = results_df.groupby('embedding_model')['coherence'].agg(['mean', 'std'])\n",
    "fig.add_trace(\n",
    "    go.Bar(x=emb_comparison.index, y=emb_comparison['mean'], \n",
    "           error_y=dict(type='data', array=emb_comparison['std']),\n",
    "           name='Coherence', showlegend=False),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# 3. Topics vs Coherence\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=results_df['n_topics'], y=results_df['coherence'],\n",
    "               mode='markers', name='Configs', showlegend=False,\n",
    "               marker=dict(color=results_df['combined_score'], colorscale='Viridis')),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# 4. Outlier % vs Coherence\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=results_df['outlier_pct'], y=results_df['coherence'],\n",
    "               mode='markers', name='Configs', showlegend=False,\n",
    "               marker=dict(color=results_df['combined_score'], colorscale='Viridis')),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=700, title_text=\"Hyperparameter Analysis\")\n",
    "fig.show()\n",
    "\n",
    "fig.write_html(f\"{PROJECT_PATH}/results/hyperparameter_analysis.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9474fb",
   "metadata": {},
   "source": [
    "## 5. Best Model Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ec3d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"BEST MODEL CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nEmbedding Model: {best_params['embedding_model']}\")\n",
    "print(f\"min_cluster_size: {best_params['min_cluster_size']}\")\n",
    "print(f\"n_neighbors: {best_params['n_neighbors']}\")\n",
    "print(f\"n_components: {best_params['n_components']}\")\n",
    "print(f\"\\nCombined Score: {best_score:.4f}\")\n",
    "\n",
    "# Get best model metrics\n",
    "best_metrics = results_df.iloc[0]\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"  Topics: {int(best_metrics['n_topics'])}\")\n",
    "print(f\"  Coherence (NPMI): {best_metrics['coherence']:.4f}\")\n",
    "print(f\"  Diversity: {best_metrics['diversity']:.4f}\")\n",
    "print(f\"  Outlier %: {best_metrics['outlier_pct']:.1f}%\")\n",
    "print(f\"  Silhouette: {best_metrics['silhouette']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a002f5b",
   "metadata": {},
   "source": [
    "## 6. Outlier Reduction\n",
    "\n",
    "BERTopic can reassign outliers to their nearest topics using various strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccadb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count outliers before\n",
    "n_outliers_before = sum(1 for t in best_topics if t == -1)\n",
    "print(f\"Outliers before reduction: {n_outliers_before} ({100*n_outliers_before/len(best_topics):.1f}%)\")\n",
    "\n",
    "# Reduce outliers using c-TF-IDF strategy\n",
    "print(\"\\nReducing outliers using c-TF-IDF similarity...\")\n",
    "new_topics = best_model.reduce_outliers(\n",
    "    documents, \n",
    "    best_topics,\n",
    "    strategy=\"c-tf-idf\",\n",
    "    threshold=0.1\n",
    ")\n",
    "\n",
    "# Update model with new topics\n",
    "best_model.update_topics(documents, topics=new_topics)\n",
    "\n",
    "# Count outliers after\n",
    "n_outliers_after = sum(1 for t in new_topics if t == -1)\n",
    "print(f\"Outliers after reduction: {n_outliers_after} ({100*n_outliers_after/len(new_topics):.1f}%)\")\n",
    "print(f\"Outliers reduced by: {n_outliers_before - n_outliers_after} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c1241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recompute metrics after outlier reduction\n",
    "final_metrics = compute_metrics(\n",
    "    best_model, documents, best_embeddings, new_topics,\n",
    "    tokenized_docs, dictionary\n",
    ")\n",
    "\n",
    "print(\"\\nMetrics After Outlier Reduction:\")\n",
    "print(f\"  Topics: {final_metrics['n_topics']}\")\n",
    "print(f\"  Coherence (NPMI): {final_metrics['coherence']:.4f}\")\n",
    "print(f\"  Diversity: {final_metrics['diversity']:.4f}\")\n",
    "print(f\"  Outlier %: {final_metrics['outlier_pct']:.1f}%\")\n",
    "print(f\"  Silhouette: {final_metrics['silhouette']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabdc9c5",
   "metadata": {},
   "source": [
    "## 7. Embedding Model Comparison Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5333716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare embedding models\n",
    "emb_summary = results_df.groupby('embedding_model').agg({\n",
    "    'coherence': ['mean', 'std', 'max'],\n",
    "    'diversity': ['mean', 'max'],\n",
    "    'n_topics': ['mean', 'min', 'max'],\n",
    "    'outlier_pct': ['mean', 'min']\n",
    "}).round(4)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EMBEDDING MODEL COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(emb_summary.to_string())\n",
    "\n",
    "# Winner determination\n",
    "mpnet_best = results_df[results_df['embedding_model'] == 'mpnet']['coherence'].max()\n",
    "minilm_best = results_df[results_df['embedding_model'] == 'minilm']['coherence'].max()\n",
    "\n",
    "print(f\"\\nðŸ† Best MPNet coherence: {mpnet_best:.4f}\")\n",
    "print(f\"ðŸ† Best MiniLM coherence: {minilm_best:.4f}\")\n",
    "print(f\"\\n{'MPNet' if mpnet_best > minilm_best else 'MiniLM'} wins by {abs(mpnet_best - minilm_best):.4f}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeb8c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: MPNet vs MiniLM\n",
    "fig = go.Figure()\n",
    "\n",
    "for emb in ['mpnet', 'minilm']:\n",
    "    subset = results_df[results_df['embedding_model'] == emb]\n",
    "    fig.add_trace(go.Box(\n",
    "        y=subset['coherence'],\n",
    "        name=emb.upper(),\n",
    "        boxpoints='all',\n",
    "        jitter=0.3\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Coherence Score Distribution: MPNet vs MiniLM',\n",
    "    yaxis_title='Coherence (NPMI)',\n",
    "    showlegend=False\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "fig.write_html(f\"{PROJECT_PATH}/results/embedding_model_comparison.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9132795",
   "metadata": {},
   "source": [
    "## 8. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e675fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "best_model_path = f\"{PROJECT_PATH}/models/bertopic_best_model\"\n",
    "os.makedirs(best_model_path, exist_ok=True)\n",
    "\n",
    "best_model.save(\n",
    "    best_model_path,\n",
    "    serialization=\"safetensors\",\n",
    "    save_ctfidf=True,\n",
    "    save_embedding_model=False\n",
    ")\n",
    "\n",
    "print(f\"Best model saved to {best_model_path}\")\n",
    "\n",
    "# Save best configuration\n",
    "best_config = {\n",
    "    'params': best_params,\n",
    "    'metrics': {\n",
    "        'n_topics': int(final_metrics['n_topics']),\n",
    "        'coherence_npmi': float(final_metrics['coherence']),\n",
    "        'diversity': float(final_metrics['diversity']),\n",
    "        'outlier_pct': float(final_metrics['outlier_pct']),\n",
    "        'silhouette': float(final_metrics['silhouette'])\n",
    "    },\n",
    "    'outlier_reduction': {\n",
    "        'before': n_outliers_before,\n",
    "        'after': n_outliers_after,\n",
    "        'strategy': 'c-tf-idf'\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"{PROJECT_PATH}/results/best_config.json\", 'w') as f:\n",
    "    json.dump(best_config, f, indent=2)\n",
    "\n",
    "print(\"Configuration saved to best_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e244442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save topic assignments with best model\n",
    "results_df_final = df.copy()\n",
    "results_df_final['topic'] = new_topics\n",
    "\n",
    "# Add topic labels\n",
    "topic_info = best_model.get_topic_info()\n",
    "topic_labels = {row['Topic']: row['Name'] for _, row in topic_info.iterrows()}\n",
    "results_df_final['topic_name'] = results_df_final['topic'].map(topic_labels)\n",
    "\n",
    "results_df_final.to_csv(f\"{PROJECT_PATH}/results/topic_assignments_best.csv\", index=False)\n",
    "topic_info.to_csv(f\"{PROJECT_PATH}/results/topic_info_best.csv\", index=False)\n",
    "\n",
    "print(\"Topic assignments saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3ef915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and save 2D embeddings for best model\n",
    "print(\"Computing 2D UMAP projection...\")\n",
    "umap_2d = UMAP(n_components=2, min_dist=0.1, metric='cosine', random_state=42)\n",
    "embeddings_2d = umap_2d.fit_transform(best_embeddings)\n",
    "\n",
    "np.save(f\"{PROJECT_PATH}/data/embeddings/embeddings_2d_best.npy\", embeddings_2d)\n",
    "print(\"2D embeddings saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689eb91a",
   "metadata": {},
   "source": [
    "## 9. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f93c44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive report\n",
    "report = f\"\"\"\n",
    "{'='*70}\n",
    "HYPERPARAMETER TUNING & MODEL COMPARISON REPORT\n",
    "Project: Topic Modeling arXiv cs.AI with BERTopic\n",
    "Team: Pavan Chauhan, Vedanta Nayak\n",
    "{'='*70}\n",
    "\n",
    "HYPERPARAMETER SEARCH\n",
    "{'-'*50}\n",
    "Total configurations tested: {len(results_df)}\n",
    "Embedding models: MPNet, MiniLM\n",
    "min_cluster_size: [10, 15, 20, 30, 50]\n",
    "n_neighbors: [10, 15, 25]\n",
    "n_components: [5, 10]\n",
    "\n",
    "EMBEDDING MODEL COMPARISON\n",
    "{'-'*50}\n",
    "MPNet (all-mpnet-base-v2):\n",
    "  - Best coherence: {results_df[results_df['embedding_model']=='mpnet']['coherence'].max():.4f}\n",
    "  - Mean coherence: {results_df[results_df['embedding_model']=='mpnet']['coherence'].mean():.4f}\n",
    "  - Parameters: 110M\n",
    "\n",
    "MiniLM (all-MiniLM-L6-v2):\n",
    "  - Best coherence: {results_df[results_df['embedding_model']=='minilm']['coherence'].max():.4f}\n",
    "  - Mean coherence: {results_df[results_df['embedding_model']=='minilm']['coherence'].mean():.4f}\n",
    "  - Parameters: 22M (5x smaller)\n",
    "\n",
    "BEST MODEL\n",
    "{'-'*50}\n",
    "Embedding: {best_params['embedding_model'].upper()}\n",
    "min_cluster_size: {best_params['min_cluster_size']}\n",
    "n_neighbors: {best_params['n_neighbors']}\n",
    "n_components: {best_params['n_components']}\n",
    "\n",
    "OUTLIER REDUCTION\n",
    "{'-'*50}\n",
    "Strategy: c-TF-IDF similarity\n",
    "Before: {n_outliers_before} outliers ({100*n_outliers_before/len(documents):.1f}%)\n",
    "After: {n_outliers_after} outliers ({100*n_outliers_after/len(documents):.1f}%)\n",
    "Improvement: {n_outliers_before - n_outliers_after} documents reassigned\n",
    "\n",
    "FINAL METRICS\n",
    "{'-'*50}\n",
    "Topics: {final_metrics['n_topics']}\n",
    "Coherence (NPMI): {final_metrics['coherence']:.4f}\n",
    "Topic Diversity: {final_metrics['diversity']:.4f} ({final_metrics['diversity']*100:.1f}%)\n",
    "Silhouette Score: {final_metrics['silhouette']:.4f}\n",
    "Outlier %: {final_metrics['outlier_pct']:.1f}%\n",
    "\n",
    "{'='*70}\n",
    "\"\"\"\n",
    "\n",
    "print(report)\n",
    "\n",
    "with open(f\"{PROJECT_PATH}/results/hyperparameter_tuning_report.txt\", 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"Report saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8468f6d3",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has:\n",
    "1. âœ… Compared MPNet vs MiniLM embeddings\n",
    "2. âœ… Performed grid search over 60 hyperparameter combinations\n",
    "3. âœ… Identified optimal configuration\n",
    "4. âœ… Applied outlier reduction\n",
    "5. âœ… Saved best model and results\n",
    "\n",
    "**Next step:** Run `04_evaluation.ipynb` for full evaluation with LDA baseline comparison."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
