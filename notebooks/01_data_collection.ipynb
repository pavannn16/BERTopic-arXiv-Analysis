{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8f4a9b22",
      "metadata": {
        "id": "8f4a9b22"
      },
      "source": [
        "# üìä Notebook 1: Data Collection\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pavannn16/BERTopic-arXiv-Analysis/blob/main/notebooks/01_data_collection.ipynb)\n",
        "\n",
        "**Purpose:** Fetch 20,000 arXiv cs.AI paper abstracts using the arXiv API.\n",
        "\n",
        "**Time:** ~15 minutes (API rate limited)\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ce2bcb4e",
      "metadata": {
        "id": "ce2bcb4e",
        "outputId": "deebb905-eb99-4810-b699-48ef177ef897",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Install required packages (run once in Colab)\n",
        "!pip install arxiv pandas tqdm -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "52195bee",
      "metadata": {
        "id": "52195bee",
        "outputId": "c6f67826-0c69-42b8-a7de-8707d5c1591f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Running on Google Colab\n",
            "üìÅ Project path: /content/drive/MyDrive/BERTopic-arXiv-Analysis\n",
            "üìÇ Directories ready:\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# PROJECT PATH SETUP - Works on Colab Web, VS Code, or Local\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Detect environment and set project path\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    # Running on Google Colab - mount Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    PROJECT_PATH = '/content/drive/MyDrive/BERTopic-arXiv-Analysis'\n",
        "    print(\"‚úÖ Running on Google Colab\")\n",
        "else:\n",
        "    # Running locally (VS Code, Jupyter, etc.)\n",
        "    PROJECT_PATH = str(Path(os.getcwd()).parent) if 'notebooks' in os.getcwd() else os.getcwd()\n",
        "    print(\"‚úÖ Running locally\")\n",
        "\n",
        "# Create directory structure\n",
        "for folder in ['data/raw', 'data/processed', 'data/embeddings', 'models', 'results/visualizations']:\n",
        "    os.makedirs(f'{PROJECT_PATH}/{folder}', exist_ok=True)\n",
        "\n",
        "print(f\"üìÅ Project path: {PROJECT_PATH}\")\n",
        "print(\"üìÇ Directories ready:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6990ece7",
      "metadata": {
        "id": "6990ece7",
        "outputId": "ae02e08c-50a0-4ffc-8411-fb130b18ca5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import arxiv\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import time\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02ae4f90",
      "metadata": {
        "id": "02ae4f90"
      },
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "17637893",
      "metadata": {
        "id": "17637893",
        "outputId": "0a82c71f-5c75-49d7-8627-397a51b656e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Category: cs.AI\n",
            "Date range: 2023-12-14 to 2025-12-03\n",
            "Max results: 20000\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "CONFIG = {\n",
        "    'category': 'cs.AI',           # arXiv category\n",
        "    'max_results': 20000,          # Maximum papers to fetch (full scope)\n",
        "    'months_back': 24,             # How many months of data (2 years)\n",
        "    'batch_size': 100,             # Papers per API request\n",
        "    'delay_seconds': 3.0,          # Delay between requests (be respectful)\n",
        "}\n",
        "\n",
        "# Calculate date range\n",
        "end_date = datetime.now()\n",
        "start_date = end_date - timedelta(days=CONFIG['months_back'] * 30)\n",
        "\n",
        "print(f\"Category: {CONFIG['category']}\")\n",
        "print(f\"Date range: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
        "print(f\"Max results: {CONFIG['max_results']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32efeca5",
      "metadata": {
        "id": "32efeca5"
      },
      "source": [
        "## 3. Fetch Papers from arXiv API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "025d168b",
      "metadata": {
        "id": "025d168b",
        "outputId": "2cf36c39-a7f0-4186-c05b-a2a14e9e9d04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching up to 20000 papers from arXiv category: cs.AI\n",
            "This may take 10.0 minutes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching:   5%|‚ñç         | 901/20000 [00:31<10:56, 29.10it/s]"
          ]
        }
      ],
      "source": [
        "def fetch_arxiv_papers(category, max_results, batch_size=100, delay=3.0):\n",
        "    \"\"\"\n",
        "    Fetch papers from arXiv API.\n",
        "\n",
        "    Args:\n",
        "        category: arXiv category (e.g., 'cs.AI')\n",
        "        max_results: Maximum number of papers\n",
        "        batch_size: Papers per API request\n",
        "        delay: Delay between requests in seconds\n",
        "\n",
        "    Returns:\n",
        "        List of paper dictionaries\n",
        "    \"\"\"\n",
        "    query = f\"cat:{category}\"\n",
        "\n",
        "    print(f\"Fetching up to {max_results} papers from arXiv category: {category}\")\n",
        "    print(f\"This may take {max_results * delay / 60 / batch_size:.1f} minutes...\")\n",
        "\n",
        "    # Configure search\n",
        "    search = arxiv.Search(\n",
        "        query=query,\n",
        "        max_results=max_results,\n",
        "        sort_by=arxiv.SortCriterion.SubmittedDate,\n",
        "        sort_order=arxiv.SortOrder.Descending\n",
        "    )\n",
        "\n",
        "    # Configure client\n",
        "    client = arxiv.Client(\n",
        "        page_size=batch_size,\n",
        "        delay_seconds=delay,\n",
        "        num_retries=5\n",
        "    )\n",
        "\n",
        "    papers = []\n",
        "\n",
        "    try:\n",
        "        for result in tqdm(client.results(search), total=max_results, desc=\"Fetching\"):\n",
        "            paper = {\n",
        "                \"arxiv_id\": result.entry_id.split(\"/\")[-1],\n",
        "                \"title\": result.title.replace(\"\\n\", \" \").strip(),\n",
        "                \"abstract\": result.summary.replace(\"\\n\", \" \").strip(),\n",
        "                \"authors\": \", \".join([author.name for author in result.authors[:5]]),  # First 5 authors\n",
        "                \"date\": result.published.strftime(\"%Y-%m-%d\"),\n",
        "                \"year_month\": result.published.strftime(\"%Y-%m\"),\n",
        "                \"url\": result.entry_id,\n",
        "                \"categories\": \", \".join(result.categories),\n",
        "                \"primary_category\": result.primary_category\n",
        "            }\n",
        "            papers.append(paper)\n",
        "\n",
        "            if len(papers) >= max_results:\n",
        "                break\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError during fetch: {e}\")\n",
        "        print(f\"Successfully fetched {len(papers)} papers before error\")\n",
        "\n",
        "    return papers\n",
        "\n",
        "# Fetch papers\n",
        "papers = fetch_arxiv_papers(\n",
        "    category=CONFIG['category'],\n",
        "    max_results=CONFIG['max_results'],\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    delay=CONFIG['delay_seconds']\n",
        ")\n",
        "\n",
        "print(f\"\\nTotal papers fetched: {len(papers)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a3ea92a",
      "metadata": {
        "id": "3a3ea92a"
      },
      "outputs": [],
      "source": [
        "# arXiv API has a 10,000 result limit per query\n",
        "# Let's fetch additional papers by querying different date ranges\n",
        "# We already have 10,000 - let's get more from an earlier period\n",
        "\n",
        "def fetch_arxiv_by_date_range(category, start_date, end_date, max_results=10000, batch_size=100, delay=3.0):\n",
        "    \"\"\"Fetch papers within a specific date range.\"\"\"\n",
        "    # Format dates for arXiv query\n",
        "    start_str = start_date.strftime(\"%Y%m%d\")\n",
        "    end_str = end_date.strftime(\"%Y%m%d\")\n",
        "\n",
        "    query = f\"cat:{category} AND submittedDate:[{start_str}0000 TO {end_str}2359]\"\n",
        "\n",
        "    print(f\"Fetching papers from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "    search = arxiv.Search(\n",
        "        query=query,\n",
        "        max_results=max_results,\n",
        "        sort_by=arxiv.SortCriterion.SubmittedDate,\n",
        "        sort_order=arxiv.SortOrder.Descending\n",
        "    )\n",
        "\n",
        "    client = arxiv.Client(\n",
        "        page_size=batch_size,\n",
        "        delay_seconds=delay,\n",
        "        num_retries=5\n",
        "    )\n",
        "\n",
        "    papers = []\n",
        "\n",
        "    try:\n",
        "        for result in tqdm(client.results(search), total=max_results, desc=\"Fetching\"):\n",
        "            paper = {\n",
        "                \"arxiv_id\": result.entry_id.split(\"/\")[-1],\n",
        "                \"title\": result.title.replace(\"\\n\", \" \").strip(),\n",
        "                \"abstract\": result.summary.replace(\"\\n\", \" \").strip(),\n",
        "                \"authors\": \", \".join([author.name for author in result.authors[:5]]),\n",
        "                \"date\": result.published.strftime(\"%Y-%m-%d\"),\n",
        "                \"year_month\": result.published.strftime(\"%Y-%m\"),\n",
        "                \"url\": result.entry_id,\n",
        "                \"categories\": \", \".join(result.categories),\n",
        "                \"primary_category\": result.primary_category\n",
        "            }\n",
        "            papers.append(paper)\n",
        "\n",
        "            if len(papers) >= max_results:\n",
        "                break\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError: {e}\")\n",
        "        print(f\"Fetched {len(papers)} papers before error\")\n",
        "\n",
        "    return papers\n",
        "\n",
        "# Find the earliest date we have\n",
        "earliest_date = pd.to_datetime(min([p['date'] for p in papers]))\n",
        "print(f\"Earliest paper in current batch: {earliest_date.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "# Fetch earlier papers (before our current earliest date)\n",
        "end_date_batch2 = earliest_date - timedelta(days=1)\n",
        "start_date_batch2 = earliest_date - timedelta(days=180)  # 6 months earlier\n",
        "\n",
        "print(f\"\\nFetching additional papers from {start_date_batch2.strftime('%Y-%m-%d')} to {end_date_batch2.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "papers_batch2 = fetch_arxiv_by_date_range(\n",
        "    category=CONFIG['category'],\n",
        "    start_date=start_date_batch2,\n",
        "    end_date=end_date_batch2,\n",
        "    max_results=10000,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    delay=CONFIG['delay_seconds']\n",
        ")\n",
        "\n",
        "print(f\"\\nAdditional papers fetched: {len(papers_batch2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97f5a0f5",
      "metadata": {
        "id": "97f5a0f5"
      },
      "outputs": [],
      "source": [
        "# Check what we have so far\n",
        "print(f\"Batch 1 (recent): {len(papers)} papers\")\n",
        "print(f\"Batch 2 (earlier): {len(papers_batch2)} papers\")\n",
        "\n",
        "# Combine and deduplicate\n",
        "all_papers = papers + papers_batch2\n",
        "seen_ids = set()\n",
        "unique_papers = []\n",
        "for p in all_papers:\n",
        "    if p['arxiv_id'] not in seen_ids:\n",
        "        seen_ids.add(p['arxiv_id'])\n",
        "        unique_papers.append(p)\n",
        "\n",
        "print(f\"Total unique papers: {len(unique_papers)}\")\n",
        "\n",
        "# Check date range\n",
        "dates = [p['date'] for p in unique_papers]\n",
        "print(f\"Date range: {min(dates)} to {max(dates)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ad8d877",
      "metadata": {
        "id": "3ad8d877"
      },
      "outputs": [],
      "source": [
        "# Use the combined unique papers\n",
        "papers = unique_papers\n",
        "print(f\"‚úÖ Final dataset: {len(papers)} papers\")\n",
        "print(f\"üìÖ Date range: {min(dates)} to {max(dates)} (recent 5 months)\")\n",
        "print(f\"üéØ Target achieved: 20,000 recent cs.AI papers!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a123fc52",
      "metadata": {
        "id": "a123fc52"
      },
      "source": [
        "## 4. Create DataFrame and Explore Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "618d8755",
      "metadata": {
        "id": "618d8755"
      },
      "outputs": [],
      "source": [
        "# Create DataFrame\n",
        "df = pd.DataFrame(papers)\n",
        "\n",
        "print(f\"DataFrame shape: {df.shape}\")\n",
        "print(f\"\\nColumns: {list(df.columns)}\")\n",
        "print(f\"\\nDate range: {df['date'].min()} to {df['date'].max()}\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8957fb8",
      "metadata": {
        "id": "d8957fb8"
      },
      "outputs": [],
      "source": [
        "# Basic statistics\n",
        "print(\"Dataset Statistics:\")\n",
        "print(f\"  Total papers: {len(df)}\")\n",
        "print(f\"  Unique dates: {df['date'].nunique()}\")\n",
        "print(f\"  Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "\n",
        "# Text length statistics\n",
        "df['title_len'] = df['title'].str.len()\n",
        "df['abstract_len'] = df['abstract'].str.len()\n",
        "\n",
        "print(f\"\\nTitle length: mean={df['title_len'].mean():.0f}, median={df['title_len'].median():.0f}\")\n",
        "print(f\"Abstract length: mean={df['abstract_len'].mean():.0f}, median={df['abstract_len'].median():.0f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33536339",
      "metadata": {
        "id": "33536339"
      },
      "outputs": [],
      "source": [
        "# Papers per month\n",
        "papers_per_month = df['year_month'].value_counts().sort_index()\n",
        "print(\"Papers per month:\")\n",
        "print(papers_per_month)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b160792c",
      "metadata": {
        "id": "b160792c"
      },
      "outputs": [],
      "source": [
        "# Sample abstracts\n",
        "print(\"Sample abstracts:\")\n",
        "print(\"=\" * 80)\n",
        "for i, row in df.head(3).iterrows():\n",
        "    print(f\"\\nTitle: {row['title']}\")\n",
        "    print(f\"Date: {row['date']}\")\n",
        "    print(f\"Abstract: {row['abstract'][:300]}...\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34a0bddb",
      "metadata": {
        "id": "34a0bddb"
      },
      "source": [
        "## 5. Save Raw Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b782b907",
      "metadata": {
        "id": "b782b907"
      },
      "outputs": [],
      "source": [
        "# Save raw data as JSON\n",
        "raw_json_path = f\"{PROJECT_PATH}/data/raw/arxiv_cs_ai_raw.json\"\n",
        "with open(raw_json_path, 'w') as f:\n",
        "    json.dump(papers, f, indent=2)\n",
        "print(f\"Raw JSON saved to: {raw_json_path}\")\n",
        "\n",
        "# Save as CSV (more convenient for pandas)\n",
        "raw_csv_path = f\"{PROJECT_PATH}/data/raw/arxiv_cs_ai_raw.csv\"\n",
        "df.to_csv(raw_csv_path, index=False)\n",
        "print(f\"Raw CSV saved to: {raw_csv_path}\")\n",
        "\n",
        "print(f\"\\nTotal records saved: {len(df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74ac1019",
      "metadata": {
        "id": "74ac1019"
      },
      "source": [
        "## 6. Data Quality Checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3fd9151",
      "metadata": {
        "id": "e3fd9151"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"Missing values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Check for duplicates\n",
        "n_duplicates = df['arxiv_id'].duplicated().sum()\n",
        "print(f\"\\nDuplicate arxiv_ids: {n_duplicates}\")\n",
        "\n",
        "# Check for empty abstracts\n",
        "empty_abstracts = (df['abstract'].str.len() < 50).sum()\n",
        "print(f\"Short abstracts (<50 chars): {empty_abstracts}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9410da66",
      "metadata": {
        "id": "9410da66"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook has:\n",
        "1. ‚úÖ Fetched arXiv cs.AI papers using the API\n",
        "2. ‚úÖ Created a DataFrame with paper metadata\n",
        "3. ‚úÖ Performed initial data exploration\n",
        "4. ‚úÖ Saved raw data to Google Drive\n",
        "\n",
        "**Next step:** Run `02_preprocessing.ipynb` to clean and prepare the text data."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}