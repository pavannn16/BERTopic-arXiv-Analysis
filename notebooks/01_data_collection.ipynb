{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f4a9b22",
   "metadata": {},
   "source": [
    "# üìä Notebook 1: Data Collection\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pavannn16/CS5660-BERTopic-arXiv/blob/main/notebooks/01_data_collection.ipynb)\n",
    "\n",
    "**Purpose:** Fetch 20,000 arXiv cs.AI paper abstracts using the arXiv API.\n",
    "\n",
    "**Time:** ~15 minutes (API rate limited)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce2bcb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once in Colab)\n",
    "!pip install arxiv pandas tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52195bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project path: /content\n",
      "Directories created:\n",
      "  ‚úì data/raw\n",
      "  ‚úì data/processed\n",
      "  ‚úì data/embeddings\n",
      "  ‚úì models\n",
      "  ‚úì results\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PROJECT PATH SETUP - Works on Colab Web, VS Code, or Local\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect environment and set project path\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    # Running on Google Colab - mount Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    PROJECT_PATH = '/content/drive/MyDrive/CS5660_BERTopic_arXiv'\n",
    "    print(\"‚úÖ Running on Google Colab\")\n",
    "else:\n",
    "    # Running locally (VS Code, Jupyter, etc.)\n",
    "    PROJECT_PATH = str(Path(os.getcwd()).parent) if 'notebooks' in os.getcwd() else os.getcwd()\n",
    "    print(\"‚úÖ Running locally\")\n",
    "\n",
    "# Create directory structure\n",
    "for folder in ['data/raw', 'data/processed', 'data/embeddings', 'models', 'results/visualizations']:\n",
    "    os.makedirs(f'{PROJECT_PATH}/{folder}', exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Project path: {PROJECT_PATH}\")\n",
    "print(\"üìÇ Directories ready:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6990ece7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import arxiv\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ae4f90",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17637893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: cs.AI\n",
      "Date range: 2023-12-13 to 2025-12-02\n",
      "Max results: 20000\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'category': 'cs.AI',           # arXiv category\n",
    "    'max_results': 20000,          # Maximum papers to fetch (full scope)\n",
    "    'months_back': 24,             # How many months of data (2 years)\n",
    "    'batch_size': 100,             # Papers per API request\n",
    "    'delay_seconds': 3.0,          # Delay between requests (be respectful)\n",
    "}\n",
    "\n",
    "# Calculate date range\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=CONFIG['months_back'] * 30)\n",
    "\n",
    "print(f\"Category: {CONFIG['category']}\")\n",
    "print(f\"Date range: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "print(f\"Max results: {CONFIG['max_results']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32efeca5",
   "metadata": {},
   "source": [
    "## 3. Fetch Papers from arXiv API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "025d168b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching up to 20000 papers from arXiv category: cs.AI\n",
      "This may take 10.0 minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10000/20000 [06:09<06:09, 27.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error during fetch: Page request resulted in HTTP 500 (https://export.arxiv.org/api/query?search_query=cat%3Acs.AI&id_list=&sortBy=submittedDate&sortOrder=descending&start=10000&max_results=100)\n",
      "Successfully fetched 10000 papers before error\n",
      "\n",
      "Total papers fetched: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def fetch_arxiv_papers(category, max_results, batch_size=100, delay=3.0):\n",
    "    \"\"\"\n",
    "    Fetch papers from arXiv API.\n",
    "    \n",
    "    Args:\n",
    "        category: arXiv category (e.g., 'cs.AI')\n",
    "        max_results: Maximum number of papers\n",
    "        batch_size: Papers per API request\n",
    "        delay: Delay between requests in seconds\n",
    "    \n",
    "    Returns:\n",
    "        List of paper dictionaries\n",
    "    \"\"\"\n",
    "    query = f\"cat:{category}\"\n",
    "    \n",
    "    print(f\"Fetching up to {max_results} papers from arXiv category: {category}\")\n",
    "    print(f\"This may take {max_results * delay / 60 / batch_size:.1f} minutes...\")\n",
    "    \n",
    "    # Configure search\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "    \n",
    "    # Configure client\n",
    "    client = arxiv.Client(\n",
    "        page_size=batch_size,\n",
    "        delay_seconds=delay,\n",
    "        num_retries=5\n",
    "    )\n",
    "    \n",
    "    papers = []\n",
    "    \n",
    "    try:\n",
    "        for result in tqdm(client.results(search), total=max_results, desc=\"Fetching\"):\n",
    "            paper = {\n",
    "                \"arxiv_id\": result.entry_id.split(\"/\")[-1],\n",
    "                \"title\": result.title.replace(\"\\n\", \" \").strip(),\n",
    "                \"abstract\": result.summary.replace(\"\\n\", \" \").strip(),\n",
    "                \"authors\": \", \".join([author.name for author in result.authors[:5]]),  # First 5 authors\n",
    "                \"date\": result.published.strftime(\"%Y-%m-%d\"),\n",
    "                \"year_month\": result.published.strftime(\"%Y-%m\"),\n",
    "                \"url\": result.entry_id,\n",
    "                \"categories\": \", \".join(result.categories),\n",
    "                \"primary_category\": result.primary_category\n",
    "            }\n",
    "            papers.append(paper)\n",
    "            \n",
    "            if len(papers) >= max_results:\n",
    "                break\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during fetch: {e}\")\n",
    "        print(f\"Successfully fetched {len(papers)} papers before error\")\n",
    "    \n",
    "    return papers\n",
    "\n",
    "# Fetch papers\n",
    "papers = fetch_arxiv_papers(\n",
    "    category=CONFIG['category'],\n",
    "    max_results=CONFIG['max_results'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    delay=CONFIG['delay_seconds']\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal papers fetched: {len(papers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a3ea92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earliest paper in current batch: 2025-09-25\n",
      "\n",
      "Fetching additional papers from 2025-03-29 to 2025-09-24\n",
      "Fetching papers from 2025-03-29 to 2025-09-24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 9999/10000 [05:37<00:00, 29.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Additional papers fetched: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# arXiv API has a 10,000 result limit per query\n",
    "# Let's fetch additional papers by querying different date ranges\n",
    "# We already have 10,000 - let's get more from an earlier period\n",
    "\n",
    "def fetch_arxiv_by_date_range(category, start_date, end_date, max_results=10000, batch_size=100, delay=3.0):\n",
    "    \"\"\"Fetch papers within a specific date range.\"\"\"\n",
    "    # Format dates for arXiv query\n",
    "    start_str = start_date.strftime(\"%Y%m%d\")\n",
    "    end_str = end_date.strftime(\"%Y%m%d\")\n",
    "    \n",
    "    query = f\"cat:{category} AND submittedDate:[{start_str}0000 TO {end_str}2359]\"\n",
    "    \n",
    "    print(f\"Fetching papers from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "    \n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "    \n",
    "    client = arxiv.Client(\n",
    "        page_size=batch_size,\n",
    "        delay_seconds=delay,\n",
    "        num_retries=5\n",
    "    )\n",
    "    \n",
    "    papers = []\n",
    "    \n",
    "    try:\n",
    "        for result in tqdm(client.results(search), total=max_results, desc=\"Fetching\"):\n",
    "            paper = {\n",
    "                \"arxiv_id\": result.entry_id.split(\"/\")[-1],\n",
    "                \"title\": result.title.replace(\"\\n\", \" \").strip(),\n",
    "                \"abstract\": result.summary.replace(\"\\n\", \" \").strip(),\n",
    "                \"authors\": \", \".join([author.name for author in result.authors[:5]]),\n",
    "                \"date\": result.published.strftime(\"%Y-%m-%d\"),\n",
    "                \"year_month\": result.published.strftime(\"%Y-%m\"),\n",
    "                \"url\": result.entry_id,\n",
    "                \"categories\": \", \".join(result.categories),\n",
    "                \"primary_category\": result.primary_category\n",
    "            }\n",
    "            papers.append(paper)\n",
    "            \n",
    "            if len(papers) >= max_results:\n",
    "                break\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "        print(f\"Fetched {len(papers)} papers before error\")\n",
    "    \n",
    "    return papers\n",
    "\n",
    "# Find the earliest date we have\n",
    "earliest_date = pd.to_datetime(min([p['date'] for p in papers]))\n",
    "print(f\"Earliest paper in current batch: {earliest_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Fetch earlier papers (before our current earliest date)\n",
    "end_date_batch2 = earliest_date - timedelta(days=1)\n",
    "start_date_batch2 = earliest_date - timedelta(days=180)  # 6 months earlier\n",
    "\n",
    "print(f\"\\nFetching additional papers from {start_date_batch2.strftime('%Y-%m-%d')} to {end_date_batch2.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "papers_batch2 = fetch_arxiv_by_date_range(\n",
    "    category=CONFIG['category'],\n",
    "    start_date=start_date_batch2,\n",
    "    end_date=end_date_batch2,\n",
    "    max_results=10000,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    delay=CONFIG['delay_seconds']\n",
    ")\n",
    "\n",
    "print(f\"\\nAdditional papers fetched: {len(papers_batch2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97f5a0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 (recent): 10000 papers\n",
      "Batch 2 (earlier): 10000 papers\n",
      "Total unique papers: 19900\n",
      "Date range: 2025-07-01 to 2025-12-01\n"
     ]
    }
   ],
   "source": [
    "# Check what we have so far\n",
    "print(f\"Batch 1 (recent): {len(papers)} papers\")\n",
    "print(f\"Batch 2 (earlier): {len(papers_batch2)} papers\")\n",
    "\n",
    "# Combine and deduplicate\n",
    "all_papers = papers + papers_batch2\n",
    "seen_ids = set()\n",
    "unique_papers = []\n",
    "for p in all_papers:\n",
    "    if p['arxiv_id'] not in seen_ids:\n",
    "        seen_ids.add(p['arxiv_id'])\n",
    "        unique_papers.append(p)\n",
    "\n",
    "print(f\"Total unique papers: {len(unique_papers)}\")\n",
    "\n",
    "# Check date range\n",
    "dates = [p['date'] for p in unique_papers]\n",
    "print(f\"Date range: {min(dates)} to {max(dates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3ad8d877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Final dataset: 19900 papers\n",
      "üìÖ Date range: 2025-07-01 to 2025-12-01 (recent 5 months)\n",
      "üéØ Target achieved: 20,000 recent cs.AI papers!\n"
     ]
    }
   ],
   "source": [
    "# Use the combined unique papers\n",
    "papers = unique_papers\n",
    "print(f\"‚úÖ Final dataset: {len(papers)} papers\")\n",
    "print(f\"üìÖ Date range: {min(dates)} to {max(dates)} (recent 5 months)\")\n",
    "print(f\"üéØ Target achieved: 20,000 recent cs.AI papers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a123fc52",
   "metadata": {},
   "source": [
    "## 4. Create DataFrame and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "618d8755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (19900, 9)\n",
      "\n",
      "Columns: ['arxiv_id', 'title', 'abstract', 'authors', 'date', 'year_month', 'url', 'categories', 'primary_category']\n",
      "\n",
      "Date range: 2025-07-01 to 2025-12-01\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-56338db2-015e-4e55-b433-0adb29b49ea7\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "      <th>year_month</th>\n",
       "      <th>url</th>\n",
       "      <th>categories</th>\n",
       "      <th>primary_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2512.01107v1</td>\n",
       "      <td>Foundation Priors</td>\n",
       "      <td>Foundation models, and in particular large lan...</td>\n",
       "      <td>Sanjog Misra</td>\n",
       "      <td>2025-11-30</td>\n",
       "      <td>2025-11</td>\n",
       "      <td>http://arxiv.org/abs/2512.01107v1</td>\n",
       "      <td>cs.AI, econ.EM, stat.ML</td>\n",
       "      <td>cs.AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2512.01105v1</td>\n",
       "      <td>Supporting Productivity Skill Development in C...</td>\n",
       "      <td>College students often face academic challenge...</td>\n",
       "      <td>Himanshi Lalwani, Hanan Salam</td>\n",
       "      <td>2025-11-30</td>\n",
       "      <td>2025-11</td>\n",
       "      <td>http://arxiv.org/abs/2512.01105v1</td>\n",
       "      <td>cs.RO, cs.AI, cs.HC</td>\n",
       "      <td>cs.RO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2512.01099v1</td>\n",
       "      <td>Energy-Aware Data-Driven Model Selection in LL...</td>\n",
       "      <td>As modern artificial intelligence (AI) systems...</td>\n",
       "      <td>Daria Smirnova, Hamid Nasiri, Marta Adamska, Z...</td>\n",
       "      <td>2025-11-30</td>\n",
       "      <td>2025-11</td>\n",
       "      <td>http://arxiv.org/abs/2512.01099v1</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>cs.AI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2512.01097v1</td>\n",
       "      <td>Discriminative classification with generative ...</td>\n",
       "      <td>We introduce Smart Bayes, a new classification...</td>\n",
       "      <td>Zachary Terner, Alexander Petersen, Yuedong Wang</td>\n",
       "      <td>2025-11-30</td>\n",
       "      <td>2025-11</td>\n",
       "      <td>http://arxiv.org/abs/2512.01097v1</td>\n",
       "      <td>stat.ML, cs.AI, cs.LG, stat.CO, stat.ME</td>\n",
       "      <td>stat.ML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2512.01095v1</td>\n",
       "      <td>CycliST: A Video Language Model Benchmark for ...</td>\n",
       "      <td>We present CycliST, a novel benchmark dataset ...</td>\n",
       "      <td>Simon Kohaut, Daniel Ochs, Shun Zhang, Benedic...</td>\n",
       "      <td>2025-11-30</td>\n",
       "      <td>2025-11</td>\n",
       "      <td>http://arxiv.org/abs/2512.01095v1</td>\n",
       "      <td>cs.CV, cs.AI, cs.LG</td>\n",
       "      <td>cs.CV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "      \n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-56338db2-015e-4e55-b433-0adb29b49ea7')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "      \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-56338db2-015e-4e55-b433-0adb29b49ea7 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-56338db2-015e-4e55-b433-0adb29b49ea7');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "  \n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       arxiv_id                                              title  \\\n",
       "0  2512.01107v1                                  Foundation Priors   \n",
       "1  2512.01105v1  Supporting Productivity Skill Development in C...   \n",
       "2  2512.01099v1  Energy-Aware Data-Driven Model Selection in LL...   \n",
       "3  2512.01097v1  Discriminative classification with generative ...   \n",
       "4  2512.01095v1  CycliST: A Video Language Model Benchmark for ...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Foundation models, and in particular large lan...   \n",
       "1  College students often face academic challenge...   \n",
       "2  As modern artificial intelligence (AI) systems...   \n",
       "3  We introduce Smart Bayes, a new classification...   \n",
       "4  We present CycliST, a novel benchmark dataset ...   \n",
       "\n",
       "                                             authors        date year_month  \\\n",
       "0                                       Sanjog Misra  2025-11-30    2025-11   \n",
       "1                      Himanshi Lalwani, Hanan Salam  2025-11-30    2025-11   \n",
       "2  Daria Smirnova, Hamid Nasiri, Marta Adamska, Z...  2025-11-30    2025-11   \n",
       "3   Zachary Terner, Alexander Petersen, Yuedong Wang  2025-11-30    2025-11   \n",
       "4  Simon Kohaut, Daniel Ochs, Shun Zhang, Benedic...  2025-11-30    2025-11   \n",
       "\n",
       "                                 url                               categories  \\\n",
       "0  http://arxiv.org/abs/2512.01107v1                  cs.AI, econ.EM, stat.ML   \n",
       "1  http://arxiv.org/abs/2512.01105v1                      cs.RO, cs.AI, cs.HC   \n",
       "2  http://arxiv.org/abs/2512.01099v1                                    cs.AI   \n",
       "3  http://arxiv.org/abs/2512.01097v1  stat.ML, cs.AI, cs.LG, stat.CO, stat.ME   \n",
       "4  http://arxiv.org/abs/2512.01095v1                      cs.CV, cs.AI, cs.LG   \n",
       "\n",
       "  primary_category  \n",
       "0            cs.AI  \n",
       "1            cs.RO  \n",
       "2            cs.AI  \n",
       "3          stat.ML  \n",
       "4            cs.CV  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame(papers)\n",
    "\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nDate range: {df['date'].min()} to {df['date'].max()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8957fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics:\n",
      "  Total papers: 19900\n",
      "  Unique dates: 154\n",
      "  Date range: 2025-07-01 to 2025-12-01\n",
      "\n",
      "Title length: mean=83, median=83\n",
      "Abstract length: mean=1339, median=1340\n"
     ]
    }
   ],
   "source": [
    "# Basic statistics\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"  Total papers: {len(df)}\")\n",
    "print(f\"  Unique dates: {df['date'].nunique()}\")\n",
    "print(f\"  Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "\n",
    "# Text length statistics\n",
    "df['title_len'] = df['title'].str.len()\n",
    "df['abstract_len'] = df['abstract'].str.len()\n",
    "\n",
    "print(f\"\\nTitle length: mean={df['title_len'].mean():.0f}, median={df['title_len'].median():.0f}\")\n",
    "print(f\"Abstract length: mean={df['abstract_len'].mean():.0f}, median={df['abstract_len'].median():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33536339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers per month:\n",
      "year_month\n",
      "2025-07    3375\n",
      "2025-08    3819\n",
      "2025-09    4135\n",
      "2025-10    4821\n",
      "2025-11    3734\n",
      "2025-12      16\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Papers per month\n",
    "papers_per_month = df['year_month'].value_counts().sort_index()\n",
    "print(\"Papers per month:\")\n",
    "print(papers_per_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b160792c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample abstracts:\n",
      "================================================================================\n",
      "\n",
      "Title: Foundation Priors\n",
      "Date: 2025-11-30\n",
      "Abstract: Foundation models, and in particular large language models, can generate highly informative responses, prompting growing interest in using these ''synthetic'' outputs as data in empirical research and decision-making. This paper introduces the idea of a foundation prior, which shows that model-gener...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Supporting Productivity Skill Development in College Students through Social Robot Coaching: A Proof-of-Concept\n",
      "Date: 2025-11-30\n",
      "Abstract: College students often face academic challenges that hamper their productivity and well-being. Although self-help books and productivity apps are popular, they often fall short. Books provide generalized, non-interactive guidance, and apps are not inherently educational and can hinder the developmen...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Energy-Aware Data-Driven Model Selection in LLM-Orchestrated AI Systems\n",
      "Date: 2025-11-30\n",
      "Abstract: As modern artificial intelligence (AI) systems become more advanced and capable, they can leverage a wide range of tools and models to perform complex tasks. Today, the task of orchestrating these models is often performed by Large Language Models (LLMs) that rely on qualitative descriptions of mode...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Sample abstracts\n",
    "print(\"Sample abstracts:\")\n",
    "print(\"=\" * 80)\n",
    "for i, row in df.head(3).iterrows():\n",
    "    print(f\"\\nTitle: {row['title']}\")\n",
    "    print(f\"Date: {row['date']}\")\n",
    "    print(f\"Abstract: {row['abstract'][:300]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a0bddb",
   "metadata": {},
   "source": [
    "## 5. Save Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b782b907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw JSON saved to: /content/data/raw/arxiv_cs_ai_raw.json\n",
      "Raw CSV saved to: /content/data/raw/arxiv_cs_ai_raw.csv\n",
      "\n",
      "Total records saved: 19900\n"
     ]
    }
   ],
   "source": [
    "# Save raw data as JSON\n",
    "raw_json_path = f\"{PROJECT_PATH}/data/raw/arxiv_cs_ai_raw.json\"\n",
    "with open(raw_json_path, 'w') as f:\n",
    "    json.dump(papers, f, indent=2)\n",
    "print(f\"Raw JSON saved to: {raw_json_path}\")\n",
    "\n",
    "# Save as CSV (more convenient for pandas)\n",
    "raw_csv_path = f\"{PROJECT_PATH}/data/raw/arxiv_cs_ai_raw.csv\"\n",
    "df.to_csv(raw_csv_path, index=False)\n",
    "print(f\"Raw CSV saved to: {raw_csv_path}\")\n",
    "\n",
    "print(f\"\\nTotal records saved: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ac1019",
   "metadata": {},
   "source": [
    "## 6. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3fd9151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "arxiv_id            0\n",
      "title               0\n",
      "abstract            0\n",
      "authors             0\n",
      "date                0\n",
      "year_month          0\n",
      "url                 0\n",
      "categories          0\n",
      "primary_category    0\n",
      "title_len           0\n",
      "abstract_len        0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate arxiv_ids: 0\n",
      "Short abstracts (<50 chars): 0\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "n_duplicates = df['arxiv_id'].duplicated().sum()\n",
    "print(f\"\\nDuplicate arxiv_ids: {n_duplicates}\")\n",
    "\n",
    "# Check for empty abstracts\n",
    "empty_abstracts = (df['abstract'].str.len() < 50).sum()\n",
    "print(f\"Short abstracts (<50 chars): {empty_abstracts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9410da66",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has:\n",
    "1. ‚úÖ Fetched arXiv cs.AI papers using the API\n",
    "2. ‚úÖ Created a DataFrame with paper metadata\n",
    "3. ‚úÖ Performed initial data exploration\n",
    "4. ‚úÖ Saved raw data to Google Drive\n",
    "\n",
    "**Next step:** Run `02_preprocessing.ipynb` to clean and prepare the text data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
