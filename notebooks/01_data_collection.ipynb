{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f4a9b22",
   "metadata": {
    "id": "8f4a9b22"
   },
   "source": [
    "# ðŸ“Š Notebook 1: Data Collection\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pavannn16/BERTopic-arXiv-Analysis/blob/main/notebooks/01_data_collection.ipynb)\n",
    "\n",
    "**Purpose:** Fetch 20,000 arXiv cs.AI paper abstracts using the arXiv API.\n",
    "\n",
    "**Time:** ~15 minutes (API rate limited)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2bcb4e",
   "metadata": {
    "id": "ce2bcb4e"
   },
   "outputs": [],
   "source": [
    "# Install required packages (run once in Colab)\n",
    "!pip install arxiv pandas tqdm pyyaml -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52195bee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52195bee",
    "outputId": "fbbf94db-0773-4dbf-8fc0-fa79b596710f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded config from ../config.yaml\n",
      "ðŸ”§ Mode: INFER\n",
      "âœ… Running locally\n",
      "ðŸ“¥ Data not found locally. Downloading from public Drive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving folder 1YPE8xxSj9Gf5MTa9pFrI-3pHCaVpOhK4 data\n",
      "Retrieving folder 1IRkb2mwqf6BMMT0tdU0ProLAMDi1uM4i embeddings\n",
      "Processing file 1WGLoEK-IaYcH-uDvjZh_ZeH0zFtzezWI embeddings_2d_best.npy\n",
      "Processing file 1xf6yBYdFVMRf8h3kbG061M_P3fBaQEWj embeddings_2d.npy\n",
      "Processing file 1ofHMMTiB87fMOO8zar89e-HOk8v7DMdO embeddings_minilm.npy\n",
      "Processing file 1xs-mfO4MvVSyNV2TLlS-uoB-FV5w1Miy embeddings_mpnet.npy\n",
      "Processing file 1xN3Qn2lbF2MSAKuOE9KNnJZtSlB5wxcU embeddings.npy\n",
      "Retrieving folder 10K8lcUuSodydWM2_MxXpIOJibf6QzVoP processed\n",
      "Processing file 1lFR86ALeTJ9_wzvwgdinVriswMkmOZUp arxiv_cs_ai_processed.csv\n",
      "Processing file 1wCdKKdeB6wXcZZxb602vWHwdo2c-hn0S documents.json\n",
      "Retrieving folder 11hHWQDL0RQGvuXD3oS5GtKBvZTWQe9nl raw\n",
      "Processing file 1rJ9iNe1U6cRuUi2xNjMhOYcyCXgJHOg3 arxiv_cs_ai_raw.csv\n",
      "Processing file 1Aas5iUtPFFbO_AYV-IbovnwoHZ93vDYM arxiv_cs_ai_raw.json\n",
      "Retrieving folder 1B6gcHSmYIVr0emBpFVJDO-DhNhmp4k2W models\n",
      "Retrieving folder 1LMz386m9NUXi7zuYqtLRvrSdETz-t7Ro bertopic_best_model\n",
      "Processing file 126Mxrvd8UD2gRNVOxyly1SXTIt6-0oyP config.json\n",
      "Processing file 1fYQrveVh-Kbn134oCOMDzLcqJrR7nqb0 ctfidf_config.json\n",
      "Processing file 1fZWLfqjbIuVKuwuBHouSeQMzcxBKozSO ctfidf.safetensors\n",
      "Processing file 1-eAVU-HaZEkE1Wbpag0_FIp2yuc901oJ topic_embeddings.safetensors\n",
      "Processing file 14LxQHvCD1bqfYsZM_UVSzonFY73xCtOh topics.json\n",
      "Retrieving folder 1XqQdOXNn1d_LVUi-lWfj8mXgasoA1aD- bertopic_model\n",
      "Processing file 1YTtEYYOMwnp1wDpamA2RNTLe5ISt7PV- config.json\n",
      "Processing file 1xgRNbtaeDeCyeiiJwXb4eGlLEzoZ2FHw ctfidf_config.json\n",
      "Processing file 1ckTfsHZfFf8hMIOeq5L_RqSfYBf2qEqC ctfidf.safetensors\n",
      "Processing file 1hFDNWHCGeBP4Z6HSdKVGIUcHHWcxzbRJ topic_embeddings.safetensors\n",
      "Processing file 1MfMriwQleiDUC33GBX_BQJ5SXio0il16 topics.json\n",
      "Retrieving folder 1zw5wlqWua6ybbOdS5ivWQDVpo4Bi37OK results\n",
      "Retrieving folder 1EnoJO7OVpgKFJBfxHKPGlDIETNDwCZv- visualizations\n",
      "Processing file 1LLnJhONer6-cWfAwz5HxghI8jRoRdcia best_config.json\n",
      "Processing file 1HS6p4MhVYqwt5uJLp84ys1gBQxkh-r07 coherence_distribution.png\n",
      "Processing file 1rLh1GNGSGVye-q1kxO3X6CZdAa-u1h9z embedding_model_comparison.html\n",
      "Processing file 1MbjtmfehSi12R4gVw-Y2UjfO_npEXBoI evaluation_metrics.json\n",
      "Processing file 1U5C3f6qNWsl-gxHjehHmQWDFP1AAvRMs evaluation_report.txt\n",
      "Processing file 1nKguqCE3RAx1PVlmRnmGp_W6Uv0RGwjE hyperparameter_analysis.html\n",
      "Processing file 1N1wPlw8j07zeXddbYJu-rkSMZ9ZsmcA_ hyperparameter_search_results.csv\n",
      "Processing file 1jdvN_uRSqLNFoGiKjmNTxcY7_F5mNB-R hyperparameter_tuning_report.txt\n",
      "Processing file 1tjPJdsvO1HBEQHr0cUEguVHd8G-4eAlE interactive_topic_map.html\n",
      "Processing file 1Ilrhbd7QyHRwNdWUkfKdlx3Mzw25_2VR model_comparison.csv\n",
      "Processing file 1UMcegxz_hAtkBpMZZfthbAA3q0Wpu8Se papers_per_month.png\n",
      "Processing file 1ptup4nemBBDOpD6kujKi592vQtKXtCNI text_length_distribution.png\n",
      "Processing file 1Z0UHSwWi1BTANFY2aAEgLXoRDf6dXOS9 topic_assignments_best.csv\n",
      "Processing file 1n9tgnpTiQLuHY0tNzfwBh87ykgctBLmL topic_assignments.csv\n",
      "Processing file 16eKWYCuiVRsm7AsgZQHwpUc0EkepSd-S topic_barchart.html\n",
      "Processing file 1uDKmz6njwk4npLCJ_wLv1ipl3fXuKuCL topic_distribution_pie.html\n",
      "Processing file 12lj3x2OlaF_Eu4-T6BD510lwX1aN6Y-Q topic_heatmap.html\n",
      "Processing file 1mJaqRg9zaPEI99vRLy1nFftMp9TiVOEG topic_hierarchy.html\n",
      "Processing file 1lo42eMglPbbDKtxUfqoGaSLryz0KG1qu topic_info_best.csv\n",
      "Processing file 1_76lzv5nAxzll3EEaJWh6GEs-1s1JM4O topic_info.csv\n",
      "Processing file 1Qu6Mtnb9AJC14nxN7pN8bbvd2HUFn52H topic_keywords_barchart.html\n",
      "Processing file 1wdTvIfKyrV9J0eY-lrVsXsjWn7_zs0i1 topic_monthly_trends.html\n",
      "Processing file 1pf3rhhtTcep18WSp2De8Ij9VkBKccm2p topic_scatter_2d.html\n",
      "Processing file 1rVUlI4ZAuLxSP26QbtNtpFDamSwmbYov topic_similarity_heatmap.html\n",
      "Processing file 18fAKsTSzBPMLyksDm1mEekL2quyHI7tO topic_size_distribution.png\n",
      "Processing file 1dnk_aLh5aVSudS7M8LBGGQ9rLyXV4wAc topic_wordclouds.png\n",
      "Processing file 110i41en0n-sdIDYueksX-DGC6w36WYkL topics_over_time.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1WGLoEK-IaYcH-uDvjZh_ZeH0zFtzezWI\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/data/embeddings/embeddings_2d_best.npy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160k/160k [00:00<00:00, 105MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1xf6yBYdFVMRf8h3kbG061M_P3fBaQEWj\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/data/embeddings/embeddings_2d.npy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 160k/160k [00:00<00:00, 658kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1ofHMMTiB87fMOO8zar89e-HOk8v7DMdO\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/data/embeddings/embeddings_minilm.npy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30.7M/30.7M [00:03<00:00, 8.16MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1xs-mfO4MvVSyNV2TLlS-uoB-FV5w1Miy\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/data/embeddings/embeddings_mpnet.npy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61.4M/61.4M [00:14<00:00, 4.21MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1xN3Qn2lbF2MSAKuOE9KNnJZtSlB5wxcU\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/data/embeddings/embeddings.npy\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61.4M/61.4M [00:06<00:00, 9.16MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1lFR86ALeTJ9_wzvwgdinVriswMkmOZUp\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/data/processed/arxiv_cs_ai_processed.csv\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88.1M/88.1M [00:10<00:00, 8.11MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1wCdKKdeB6wXcZZxb602vWHwdo2c-hn0S\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/data/processed/documents.json\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.4M/28.4M [00:03<00:00, 7.89MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1rJ9iNe1U6cRuUi2xNjMhOYcyCXgJHOg3\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/data/raw/arxiv_cs_ai_raw.csv\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31.7M/31.7M [00:03<00:00, 9.14MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Aas5iUtPFFbO_AYV-IbovnwoHZ93vDYM\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/data/raw/arxiv_cs_ai_raw.json\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35.0M/35.0M [00:04<00:00, 8.41MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=126Mxrvd8UD2gRNVOxyly1SXTIt6-0oyP\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/models/bertopic_best_model/config.json\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 302/302 [00:00<00:00, 330kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1fYQrveVh-Kbn134oCOMDzLcqJrR7nqb0\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/models/bertopic_best_model/ctfidf_config.json\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.42M/2.42M [00:00<00:00, 3.45MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1fZWLfqjbIuVKuwuBHouSeQMzcxBKozSO\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/models/bertopic_best_model/ctfidf.safetensors\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.45M/6.45M [00:00<00:00, 7.10MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1-eAVU-HaZEkE1Wbpag0_FIp2yuc901oJ\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/models/bertopic_best_model/topic_embeddings.safetensors\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 283k/283k [00:00<00:00, 935kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=14LxQHvCD1bqfYsZM_UVSzonFY73xCtOh\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/models/bertopic_best_model/topics.json\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 224k/224k [00:00<00:00, 683kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1YTtEYYOMwnp1wDpamA2RNTLe5ISt7PV-\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/models/bertopic_model/config.json\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 300/300 [00:00<00:00, 691kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1xgRNbtaeDeCyeiiJwXb4eGlLEzoZ2FHw\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/models/bertopic_model/ctfidf_config.json\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.60M/1.60M [00:00<00:00, 2.91MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1ckTfsHZfFf8hMIOeq5L_RqSfYBf2qEqC\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/models/bertopic_model/ctfidf.safetensors\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.34M/9.34M [00:01<00:00, 7.52MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1hFDNWHCGeBP4Z6HSdKVGIUcHHWcxzbRJ\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/models/bertopic_model/topic_embeddings.safetensors\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 525k/525k [00:00<00:00, 1.14MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1MfMriwQleiDUC33GBX_BQJ5SXio0il16\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/models/bertopic_model/topics.json\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 295k/295k [00:00<00:00, 765kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1LLnJhONer6-cWfAwz5HxghI8jRoRdcia\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/results/best_config.json\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 419/419 [00:00<00:00, 1.33MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1HS6p4MhVYqwt5uJLp84ys1gBQxkh-r07\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/results/coherence_distribution.png\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36.7k/36.7k [00:00<00:00, 336kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1rLh1GNGSGVye-q1kxO3X6CZdAa-u1h9z\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/results/embedding_model_comparison.html\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.57M/4.57M [00:00<00:00, 2.01GB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1MbjtmfehSi12R4gVw-Y2UjfO_npEXBoI\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/results/evaluation_metrics.json\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.59k/2.59k [00:00<00:00, 3.60MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1U5C3f6qNWsl-gxHjehHmQWDFP1AAvRMs\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/results/evaluation_report.txt\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.82k/1.82k [00:00<00:00, 1.72MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1nKguqCE3RAx1PVlmRnmGp_W6Uv0RGwjE\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/results/hyperparameter_analysis.html\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.58M/4.58M [00:00<00:00, 2.10GB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1N1wPlw8j07zeXddbYJu-rkSMZ9ZsmcA_\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/results/hyperparameter_search_results.csv\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.45k/6.45k [00:00<00:00, 500kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1jdvN_uRSqLNFoGiKjmNTxcY7_F5mNB-R\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/results/hyperparameter_tuning_report.txt\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.38k/1.38k [00:00<00:00, 2.53MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1tjPJdsvO1HBEQHr0cUEguVHd8G-4eAlE\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/results/interactive_topic_map.html\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.03M/7.03M [00:00<00:00, 2.50GB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Ilrhbd7QyHRwNdWUkfKdlx3Mzw25_2VR\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/results/model_comparison.csv\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 265/265 [00:00<00:00, 386kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1UMcegxz_hAtkBpMZZfthbAA3q0Wpu8Se\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/results/papers_per_month.png\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 36.3k/36.3k [00:00<00:00, 340kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1ptup4nemBBDOpD6kujKi592vQtKXtCNI\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/results/text_length_distribution.png\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53.7k/53.7k [00:00<00:00, 406kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1Z0UHSwWi1BTANFY2aAEgLXoRDf6dXOS9\n",
      "To: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/results/topic_assignments_best.csv\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 88.8M/88.8M [00:09<00:00, 9.04MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Auto-download failed: Failed to retrieve file url:\n",
      "\n",
      "\tCannot retrieve the public link of the file. You may need to change\n",
      "\tthe permission to 'Anyone with the link', or have had many accesses.\n",
      "\tCheck FAQ in https://github.com/wkentaro/gdown?tab=readme-ov-file#faq.\n",
      "\n",
      "You may still be able to access the file from the browser:\n",
      "\n",
      "\thttps://drive.google.com/uc?id=1n9tgnpTiQLuHY0tNzfwBh87ykgctBLmL\n",
      "\n",
      "but Gdown can't. Please check connections and permissions.\n",
      "   Manual download: https://drive.google.com/drive/folders/1T3vkmvm8YbUCXCMRoroWDXJlKHfMC5Gj\n",
      "   Or set mode='train' in config.yaml to fetch fresh data from arXiv\n",
      "ðŸ“ Project path: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PROJECT SETUP - Config-based with Train/Infer Modes\n",
    "# ============================================================\n",
    "import os\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Clone repo if running on Colab\n",
    "if 'google.colab' in str(get_ipython()) and not os.path.exists('/content/BERTopic-arXiv-Analysis'):\n",
    "    !git clone https://github.com/pavannn16/BERTopic-arXiv-Analysis.git /content/BERTopic-arXiv-Analysis\n",
    "\n",
    "# Load configuration\n",
    "def load_config():\n",
    "    config_paths = ['config.yaml', '../config.yaml', '/content/BERTopic-arXiv-Analysis/config.yaml']\n",
    "    for path in config_paths:\n",
    "        if os.path.exists(path):\n",
    "            with open(path, 'r') as f:\n",
    "                return yaml.safe_load(f), path\n",
    "    return None, None\n",
    "\n",
    "config, config_path = load_config()\n",
    "if config:\n",
    "    print(f\"âœ… Loaded config from {config_path}\")\n",
    "else:\n",
    "    print(\"âš ï¸ Config not found, using defaults\")\n",
    "    config = {'mode': 'infer', 'data': {'arxiv': {'category': 'cs.AI', 'max_results': 20000}}}\n",
    "\n",
    "MODE = config.get('mode', 'infer')\n",
    "print(f\"ðŸ”§ Mode: {MODE.upper()}\")\n",
    "\n",
    "# Setup paths\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    if MODE == 'train':\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        PROJECT_PATH = '/content/drive/MyDrive/BERTopic-arXiv-Analysis'\n",
    "        print(\"âœ… TRAIN mode: Personal Drive mounted - will fetch fresh data\")\n",
    "    else:\n",
    "        PROJECT_PATH = '/content/BERTopic-arXiv-Analysis'\n",
    "        print(\"âœ… INFER mode: Using data from cloned repo\")\n",
    "        print(\"âš ï¸ Data fetching skipped in INFER mode. Change mode='train' in config.yaml to fetch new data.\")\n",
    "else:\n",
    "    # Running locally\n",
    "    PROJECT_PATH = str(Path(os.getcwd()).parent) if 'notebooks' in os.getcwd() else os.getcwd()\n",
    "    print(\"âœ… Running locally\")\n",
    "\n",
    "# Create directories\n",
    "for folder in ['data/raw', 'data/processed', 'data/embeddings', 'models', 'results/visualizations']:\n",
    "    os.makedirs(f'{PROJECT_PATH}/{folder}', exist_ok=True)\n",
    "\n",
    "print(f\"ðŸ“ Project path: {PROJECT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6990ece7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6990ece7",
    "outputId": "83618471-327f-4713-f64c-5f90509d0849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import arxiv\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ae4f90",
   "metadata": {
    "id": "02ae4f90"
   },
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17637893",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17637893",
    "outputId": "9cd374be-bda1-42d1-e786-795a2eb7ea6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: cs.AI\n",
      "Date range: 2023-12-14 to 2025-12-03\n",
      "Max results: 20000\n",
      "\n",
      "âš ï¸ INFER mode: Skipping data fetch. Data should already exist.\n",
      "   To fetch fresh data, set mode='train' in config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Configuration - loaded from config.yaml or defaults\n",
    "ARXIV_CONFIG = config.get('data', {}).get('arxiv', {})\n",
    "\n",
    "CONFIG = {\n",
    "    'category': ARXIV_CONFIG.get('category', 'cs.AI'),\n",
    "    'max_results': ARXIV_CONFIG.get('max_results', 20000),\n",
    "    'months_back': ARXIV_CONFIG.get('months_back', 24),\n",
    "    'batch_size': ARXIV_CONFIG.get('batch_size', 100),\n",
    "    'delay_seconds': ARXIV_CONFIG.get('delay_seconds', 3.0),\n",
    "}\n",
    "\n",
    "# Calculate date range\n",
    "from datetime import datetime, timedelta\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=CONFIG['months_back'] * 30)\n",
    "\n",
    "print(f\"Category: {CONFIG['category']}\")\n",
    "print(f\"Date range: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "print(f\"Max results: {CONFIG['max_results']}\")\n",
    "\n",
    "# Check mode - skip fetching in INFER mode\n",
    "if MODE == 'infer':\n",
    "    print(\"\\nâš ï¸ INFER mode: Skipping data fetch. Data should already exist.\")\n",
    "    print(\"   To fetch fresh data, set mode='train' in config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32efeca5",
   "metadata": {
    "id": "32efeca5"
   },
   "source": [
    "## 3. Fetch Papers from arXiv API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "025d168b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "025d168b",
    "outputId": "3e387bf6-ee6b-44f8-f295-01832714e462"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching up to 20000 papers from arXiv category: cs.AI\n",
      "This may take 10.0 minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10000/20000 [06:55<06:55, 24.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error during fetch: Page request resulted in HTTP 500 (https://export.arxiv.org/api/query?search_query=cat%3Acs.AI&id_list=&sortBy=submittedDate&sortOrder=descending&start=10000&max_results=100)\n",
      "Successfully fetched 10000 papers before error\n",
      "\n",
      "Total papers fetched: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def fetch_arxiv_papers(category, max_results, batch_size=100, delay=3.0):\n",
    "    \"\"\"\n",
    "    Fetch papers from arXiv API.\n",
    "\n",
    "    Args:\n",
    "        category: arXiv category (e.g., 'cs.AI')\n",
    "        max_results: Maximum number of papers\n",
    "        batch_size: Papers per API request\n",
    "        delay: Delay between requests in seconds\n",
    "\n",
    "    Returns:\n",
    "        List of paper dictionaries\n",
    "    \"\"\"\n",
    "    query = f\"cat:{category}\"\n",
    "\n",
    "    print(f\"Fetching up to {max_results} papers from arXiv category: {category}\")\n",
    "    print(f\"This may take {max_results * delay / 60 / batch_size:.1f} minutes...\")\n",
    "\n",
    "    # Configure search\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "\n",
    "    # Configure client\n",
    "    client = arxiv.Client(\n",
    "        page_size=batch_size,\n",
    "        delay_seconds=delay,\n",
    "        num_retries=5\n",
    "    )\n",
    "\n",
    "    papers = []\n",
    "\n",
    "    try:\n",
    "        for result in tqdm(client.results(search), total=max_results, desc=\"Fetching\"):\n",
    "            paper = {\n",
    "                \"arxiv_id\": result.entry_id.split(\"/\")[-1],\n",
    "                \"title\": result.title.replace(\"\\n\", \" \").strip(),\n",
    "                \"abstract\": result.summary.replace(\"\\n\", \" \").strip(),\n",
    "                \"authors\": \", \".join([author.name for author in result.authors[:5]]),  # First 5 authors\n",
    "                \"date\": result.published.strftime(\"%Y-%m-%d\"),\n",
    "                \"year_month\": result.published.strftime(\"%Y-%m\"),\n",
    "                \"url\": result.entry_id,\n",
    "                \"categories\": \", \".join(result.categories),\n",
    "                \"primary_category\": result.primary_category\n",
    "            }\n",
    "            papers.append(paper)\n",
    "\n",
    "            if len(papers) >= max_results:\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during fetch: {e}\")\n",
    "        print(f\"Successfully fetched {len(papers)} papers before error\")\n",
    "\n",
    "    return papers\n",
    "\n",
    "# Fetch papers\n",
    "papers = fetch_arxiv_papers(\n",
    "    category=CONFIG['category'],\n",
    "    max_results=CONFIG['max_results'],\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    delay=CONFIG['delay_seconds']\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal papers fetched: {len(papers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a3ea92a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3a3ea92a",
    "outputId": "2de96597-89be-478a-d646-be4574a03acf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earliest paper in current batch: 2025-09-26\n",
      "\n",
      "Fetching additional papers from 2025-03-30 to 2025-09-25\n",
      "Fetching papers from 2025-03-30 to 2025-09-25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 9999/10000 [05:36<00:00, 29.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Additional papers fetched: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# arXiv API has a 10,000 result limit per query\n",
    "# Let's fetch additional papers by querying different date ranges\n",
    "# We already have 10,000 - let's get more from an earlier period\n",
    "\n",
    "def fetch_arxiv_by_date_range(category, start_date, end_date, max_results=10000, batch_size=100, delay=3.0):\n",
    "    \"\"\"Fetch papers within a specific date range.\"\"\"\n",
    "    # Format dates for arXiv query\n",
    "    start_str = start_date.strftime(\"%Y%m%d\")\n",
    "    end_str = end_date.strftime(\"%Y%m%d\")\n",
    "\n",
    "    query = f\"cat:{category} AND submittedDate:[{start_str}0000 TO {end_str}2359]\"\n",
    "\n",
    "    print(f\"Fetching papers from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "        sort_order=arxiv.SortOrder.Descending\n",
    "    )\n",
    "\n",
    "    client = arxiv.Client(\n",
    "        page_size=batch_size,\n",
    "        delay_seconds=delay,\n",
    "        num_retries=5\n",
    "    )\n",
    "\n",
    "    papers = []\n",
    "\n",
    "    try:\n",
    "        for result in tqdm(client.results(search), total=max_results, desc=\"Fetching\"):\n",
    "            paper = {\n",
    "                \"arxiv_id\": result.entry_id.split(\"/\")[-1],\n",
    "                \"title\": result.title.replace(\"\\n\", \" \").strip(),\n",
    "                \"abstract\": result.summary.replace(\"\\n\", \" \").strip(),\n",
    "                \"authors\": \", \".join([author.name for author in result.authors[:5]]),\n",
    "                \"date\": result.published.strftime(\"%Y-%m-%d\"),\n",
    "                \"year_month\": result.published.strftime(\"%Y-%m\"),\n",
    "                \"url\": result.entry_id,\n",
    "                \"categories\": \", \".join(result.categories),\n",
    "                \"primary_category\": result.primary_category\n",
    "            }\n",
    "            papers.append(paper)\n",
    "\n",
    "            if len(papers) >= max_results:\n",
    "                break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "        print(f\"Fetched {len(papers)} papers before error\")\n",
    "\n",
    "    return papers\n",
    "\n",
    "# Find the earliest date we have\n",
    "earliest_date = pd.to_datetime(min([p['date'] for p in papers]))\n",
    "print(f\"Earliest paper in current batch: {earliest_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Fetch earlier papers (before our current earliest date)\n",
    "end_date_batch2 = earliest_date - timedelta(days=1)\n",
    "start_date_batch2 = earliest_date - timedelta(days=180)  # 6 months earlier\n",
    "\n",
    "print(f\"\\nFetching additional papers from {start_date_batch2.strftime('%Y-%m-%d')} to {end_date_batch2.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "papers_batch2 = fetch_arxiv_by_date_range(\n",
    "    category=CONFIG['category'],\n",
    "    start_date=start_date_batch2,\n",
    "    end_date=end_date_batch2,\n",
    "    max_results=10000,\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    delay=CONFIG['delay_seconds']\n",
    ")\n",
    "\n",
    "print(f\"\\nAdditional papers fetched: {len(papers_batch2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97f5a0f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97f5a0f5",
    "outputId": "3116de1b-50a1-4e4f-f512-842d13757017"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 (recent): 10000 papers\n",
      "Batch 2 (earlier): 10000 papers\n",
      "Total unique papers: 20000\n",
      "Date range: 2025-07-03 to 2025-12-02\n"
     ]
    }
   ],
   "source": [
    "# Check what we have so far\n",
    "print(f\"Batch 1 (recent): {len(papers)} papers\")\n",
    "print(f\"Batch 2 (earlier): {len(papers_batch2)} papers\")\n",
    "\n",
    "# Combine and deduplicate\n",
    "all_papers = papers + papers_batch2\n",
    "seen_ids = set()\n",
    "unique_papers = []\n",
    "for p in all_papers:\n",
    "    if p['arxiv_id'] not in seen_ids:\n",
    "        seen_ids.add(p['arxiv_id'])\n",
    "        unique_papers.append(p)\n",
    "\n",
    "print(f\"Total unique papers: {len(unique_papers)}\")\n",
    "\n",
    "# Check date range\n",
    "dates = [p['date'] for p in unique_papers]\n",
    "print(f\"Date range: {min(dates)} to {max(dates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ad8d877",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ad8d877",
    "outputId": "c7fb68ee-6678-4df5-bb6d-96acaa610f7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Final dataset: 20000 papers\n",
      "ðŸ“… Date range: 2025-07-03 to 2025-12-02 (recent 5 months)\n",
      "ðŸŽ¯ Target achieved: 20,000 recent cs.AI papers!\n"
     ]
    }
   ],
   "source": [
    "# Use the combined unique papers\n",
    "papers = unique_papers\n",
    "print(f\"âœ… Final dataset: {len(papers)} papers\")\n",
    "print(f\"ðŸ“… Date range: {min(dates)} to {max(dates)} (recent 5 months)\")\n",
    "print(f\"ðŸŽ¯ Target achieved: 20,000 recent cs.AI papers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a123fc52",
   "metadata": {
    "id": "a123fc52"
   },
   "source": [
    "## 4. Create DataFrame and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "618d8755",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "618d8755",
    "outputId": "afb02553-048e-4f6a-e098-7a17a25d23e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (20000, 9)\n",
      "\n",
      "Columns: ['arxiv_id', 'title', 'abstract', 'authors', 'date', 'year_month', 'url', 'categories', 'primary_category']\n",
      "\n",
      "Date range: 2025-07-03 to 2025-12-02\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 20000,\n  \"fields\": [\n    {\n      \"column\": \"arxiv_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20000,\n        \"samples\": [\n          \"2509.20386v1\",\n          \"2511.12449v1\",\n          \"2510.01069v1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 19999,\n        \"samples\": [\n          \"Dynamic ReAct: Scalable Tool Selection for Large-Scale MCP Environments\",\n          \"MOON2.0: Dynamic Modality-balanced Multimodal Representation Learning for E-commerce Product Understanding\",\n          \"Typed Chain-of-Thought: A Curry-Howard Framework for Verifying LLM Reasoning\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 19997,\n        \"samples\": [\n          \"This paper introduces MCTS-EP, an online learning framework that combines large language models (LLM) with Monte Carlo Tree Search (MCTS) for training embodied agents. MCTS-EP integrates three key components: MCTS-guided exploration for preference data collection, efficient multi-modal reasoning mechanism, and iterative training pipeline based on preference optimization. We theoretically prove that MCTS-EP achieves better performance bounds than conventional on-policy algorithms when the loss function is strongly convex, and demonstrate that it can be formulated as a search-enhanced variant of GAIL. MCTS-EP achieves state-of-the-art performace across serval benchmarks. In ALFWorld, it achieves 92% and 87% success rates for textual and visual tasks. In WebShop, it reaches an average reward of 0.81. MTCS-EP also reduces average interaction steps from from 18.7/19.5 to 10.2/9.9 steps in visual ALFWorld.Code available at: https://github.com/xuhang-2/Embodied-Agent-Planning\",\n          \"The integration of Artificial Intelligence (AI) into safety-critical systems introduces a new reliability paradigm: silent failures, where AI produces confident but incorrect outputs that can be dangerous. This paper introduces the Formal Assurance and Monitoring Environment (FAME), a novel framework that confronts this challenge. FAME synergizes the mathematical rigor of offline formal synthesis with the vigilance of online runtime monitoring to create a verifiable safety net around opaque AI components. We demonstrate its efficacy in an autonomous vehicle perception system, where FAME successfully detected 93.5% of critical safety violations that were otherwise silent. By contextualizing our framework within the ISO 26262 and ISO/PAS 8800 standards, we provide reliability engineers with a practical, certifiable pathway for deploying trustworthy AI. FAME represents a crucial shift from accepting probabilistic performance to enforcing provable safety in next-generation systems.\",\n          \"Retrieval-Augmented Generation (RAG) is a critical technique for grounding Large Language Models (LLMs) in factual evidence, yet evaluating RAG systems in specialized, safety-critical domains remains a significant challenge. Existing evaluation frameworks often rely on heuristic-based metrics that fail to capture domain-specific nuances and other works utilize LLM-as-a-Judge approaches that lack validated alignment with human judgment. This paper introduces RAGalyst, an automated, human-aligned agentic framework designed for the rigorous evaluation of domain-specific RAG systems. RAGalyst features an agentic pipeline that generates high-quality, synthetic question-answering (QA) datasets from source documents, incorporating an agentic filtering step to ensure data fidelity. The framework refines two key LLM-as-a-Judge metrics-Answer Correctness and Answerability-using prompt optimization to achieve a strong correlation with human annotations. Applying this framework to evaluate various RAG components across three distinct domains (military operations, cybersecurity, and bridge engineering), we find that performance is highly context-dependent. No single embedding model, LLM, or hyperparameter configuration proves universally optimal. Additionally, we provide an analysis on the most common low Answer Correctness reasons in RAG. These findings highlight the necessity of a systematic evaluation framework like RAGalyst, which empowers practitioners to uncover domain-specific trade-offs and make informed design choices for building reliable and effective RAG systems. RAGalyst is available on our Github.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 19531,\n        \"samples\": [\n          \"Liang Gong, Tommy, Wang, Sara Chaker, Yanchen Dong\",\n          \"Yu Yan, Sheng Sun, Zhe Wang, Yijun Lin, Zenghao Duan\",\n          \"Shih-Yang Liu, Xin Dong, Ximing Lu, Shizhe Diao, Mingjie Liu\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 153,\n        \"samples\": [\n          \"2025-09-09\",\n          \"2025-09-07\",\n          \"2025-08-27\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year_month\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"2025-12\",\n          \"2025-11\",\n          \"2025-07\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20000,\n        \"samples\": [\n          \"http://arxiv.org/abs/2509.20386v1\",\n          \"http://arxiv.org/abs/2511.12449v1\",\n          \"http://arxiv.org/abs/2510.01069v1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"categories\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2176,\n        \"samples\": [\n          \"cs.PF, cs.AI, cs.DC, cs.LG\",\n          \"cs.LG, cs.AI, math.RT, stat.ML\",\n          \"cs.AI, cs.CE, cs.CL, cs.CV, stat.ME\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"primary_category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 115,\n        \"samples\": [\n          \"math.HO\",\n          \"cs.AI\",\n          \"cs.AR\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-119631eb-d093-4da0-b457-10f63642dd7d\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "      <th>year_month</th>\n",
       "      <th>url</th>\n",
       "      <th>categories</th>\n",
       "      <th>primary_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2512.02987v1</td>\n",
       "      <td>Fine-Tuned Large Language Models for Logical T...</td>\n",
       "      <td>Recent advances in natural language processing...</td>\n",
       "      <td>Muyu Pan, Dheeraj Kodakandla, Mahfuza Farooque</td>\n",
       "      <td>2025-12-02</td>\n",
       "      <td>2025-12</td>\n",
       "      <td>http://arxiv.org/abs/2512.02987v1</td>\n",
       "      <td>cs.CL, cs.AI</td>\n",
       "      <td>cs.CL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2512.02978v1</td>\n",
       "      <td>Rethinking Generalized BCIs: Benchmarking 340,...</td>\n",
       "      <td>Robust decoding and classification of brain pa...</td>\n",
       "      <td>Paul Barbaste, Olivier Oullier, Xavier Vasques</td>\n",
       "      <td>2025-12-02</td>\n",
       "      <td>2025-12</td>\n",
       "      <td>http://arxiv.org/abs/2512.02978v1</td>\n",
       "      <td>q-bio.NC, cs.AI, cs.HC, cs.LG</td>\n",
       "      <td>q-bio.NC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2512.02966v1</td>\n",
       "      <td>Lumos: Let there be Language Model System Cert...</td>\n",
       "      <td>We introduce the first principled framework, L...</td>\n",
       "      <td>Isha Chaudhary, Vedaant Jain, Avaljot Singh, K...</td>\n",
       "      <td>2025-12-02</td>\n",
       "      <td>2025-12</td>\n",
       "      <td>http://arxiv.org/abs/2512.02966v1</td>\n",
       "      <td>cs.PL, cs.AI, cs.MA</td>\n",
       "      <td>cs.PL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2512.02942v1</td>\n",
       "      <td>Benchmarking Scientific Understanding and Reas...</td>\n",
       "      <td>The next frontier for video generation lies in...</td>\n",
       "      <td>Lanxiang Hu, Abhilash Shankarampeta, Yixin Hua...</td>\n",
       "      <td>2025-12-02</td>\n",
       "      <td>2025-12</td>\n",
       "      <td>http://arxiv.org/abs/2512.02942v1</td>\n",
       "      <td>cs.CV, cs.AI</td>\n",
       "      <td>cs.CV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2512.02932v1</td>\n",
       "      <td>EGGS: Exchangeable 2D/3D Gaussian Splatting fo...</td>\n",
       "      <td>Novel view synthesis (NVS) is crucial in compu...</td>\n",
       "      <td>Yancheng Zhang, Guangyu Sun, Chen Chen</td>\n",
       "      <td>2025-12-02</td>\n",
       "      <td>2025-12</td>\n",
       "      <td>http://arxiv.org/abs/2512.02932v1</td>\n",
       "      <td>cs.CV, cs.AI</td>\n",
       "      <td>cs.CV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-119631eb-d093-4da0-b457-10f63642dd7d')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-119631eb-d093-4da0-b457-10f63642dd7d button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-119631eb-d093-4da0-b457-10f63642dd7d');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-88edab04-02a7-45dc-a604-3df9621223e4\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-88edab04-02a7-45dc-a604-3df9621223e4')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-88edab04-02a7-45dc-a604-3df9621223e4 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "       arxiv_id                                              title  \\\n",
       "0  2512.02987v1  Fine-Tuned Large Language Models for Logical T...   \n",
       "1  2512.02978v1  Rethinking Generalized BCIs: Benchmarking 340,...   \n",
       "2  2512.02966v1  Lumos: Let there be Language Model System Cert...   \n",
       "3  2512.02942v1  Benchmarking Scientific Understanding and Reas...   \n",
       "4  2512.02932v1  EGGS: Exchangeable 2D/3D Gaussian Splatting fo...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Recent advances in natural language processing...   \n",
       "1  Robust decoding and classification of brain pa...   \n",
       "2  We introduce the first principled framework, L...   \n",
       "3  The next frontier for video generation lies in...   \n",
       "4  Novel view synthesis (NVS) is crucial in compu...   \n",
       "\n",
       "                                             authors        date year_month  \\\n",
       "0     Muyu Pan, Dheeraj Kodakandla, Mahfuza Farooque  2025-12-02    2025-12   \n",
       "1     Paul Barbaste, Olivier Oullier, Xavier Vasques  2025-12-02    2025-12   \n",
       "2  Isha Chaudhary, Vedaant Jain, Avaljot Singh, K...  2025-12-02    2025-12   \n",
       "3  Lanxiang Hu, Abhilash Shankarampeta, Yixin Hua...  2025-12-02    2025-12   \n",
       "4             Yancheng Zhang, Guangyu Sun, Chen Chen  2025-12-02    2025-12   \n",
       "\n",
       "                                 url                     categories  \\\n",
       "0  http://arxiv.org/abs/2512.02987v1                   cs.CL, cs.AI   \n",
       "1  http://arxiv.org/abs/2512.02978v1  q-bio.NC, cs.AI, cs.HC, cs.LG   \n",
       "2  http://arxiv.org/abs/2512.02966v1            cs.PL, cs.AI, cs.MA   \n",
       "3  http://arxiv.org/abs/2512.02942v1                   cs.CV, cs.AI   \n",
       "4  http://arxiv.org/abs/2512.02932v1                   cs.CV, cs.AI   \n",
       "\n",
       "  primary_category  \n",
       "0            cs.CL  \n",
       "1         q-bio.NC  \n",
       "2            cs.PL  \n",
       "3            cs.CV  \n",
       "4            cs.CV  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame(papers)\n",
    "\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nDate range: {df['date'].min()} to {df['date'].max()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8957fb8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d8957fb8",
    "outputId": "a7a25412-949b-4109-f382-728fc1c42f4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics:\n",
      "  Total papers: 20000\n",
      "  Unique dates: 153\n",
      "  Date range: 2025-07-03 to 2025-12-02\n",
      "\n",
      "Title length: mean=83, median=83\n",
      "Abstract length: mean=1340, median=1340\n"
     ]
    }
   ],
   "source": [
    "# Basic statistics\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"  Total papers: {len(df)}\")\n",
    "print(f\"  Unique dates: {df['date'].nunique()}\")\n",
    "print(f\"  Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "\n",
    "# Text length statistics\n",
    "df['title_len'] = df['title'].str.len()\n",
    "df['abstract_len'] = df['abstract'].str.len()\n",
    "\n",
    "print(f\"\\nTitle length: mean={df['title_len'].mean():.0f}, median={df['title_len'].median():.0f}\")\n",
    "print(f\"Abstract length: mean={df['abstract_len'].mean():.0f}, median={df['abstract_len'].median():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33536339",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33536339",
    "outputId": "91e6cd90-df2a-48f1-d5aa-5062e6971068"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers per month:\n",
      "year_month\n",
      "2025-07    3156\n",
      "2025-08    3819\n",
      "2025-09    4215\n",
      "2025-10    4821\n",
      "2025-11    3755\n",
      "2025-12     234\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Papers per month\n",
    "papers_per_month = df['year_month'].value_counts().sort_index()\n",
    "print(\"Papers per month:\")\n",
    "print(papers_per_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b160792c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b160792c",
    "outputId": "4014e4c5-1844-42dd-e48b-b92e56747d6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample abstracts:\n",
      "================================================================================\n",
      "\n",
      "Title: Fine-Tuned Large Language Models for Logical Translation: Reducing Hallucinations with Lang2Logic\n",
      "Date: 2025-12-02\n",
      "Abstract: Recent advances in natural language processing (NLP), particularly large language models (LLMs), have motivated the automatic translation of natural language statements into formal logic without human intervention. This enables automated reasoning and facilitates debugging, finding loop invariants, ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Rethinking Generalized BCIs: Benchmarking 340,000+ Unique Algorithmic Configurations for EEG Mental Command Decoding\n",
      "Date: 2025-12-02\n",
      "Abstract: Robust decoding and classification of brain patterns measured with electroencephalography (EEG) remains a major challenge for real-world (i.e. outside scientific lab and medical facilities) brain-computer interface (BCI) applications due to well documented inter- and intra-participant variability. H...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Lumos: Let there be Language Model System Certification\n",
      "Date: 2025-12-02\n",
      "Abstract: We introduce the first principled framework, Lumos, for specifying and formally certifying Language Model System (LMS) behaviors. Lumos is an imperative probabilistic programming DSL over graphs, with constructs to generate independent and identically distributed prompts for LMS. It offers a structu...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Sample abstracts\n",
    "print(\"Sample abstracts:\")\n",
    "print(\"=\" * 80)\n",
    "for i, row in df.head(3).iterrows():\n",
    "    print(f\"\\nTitle: {row['title']}\")\n",
    "    print(f\"Date: {row['date']}\")\n",
    "    print(f\"Abstract: {row['abstract'][:300]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a0bddb",
   "metadata": {
    "id": "34a0bddb"
   },
   "source": [
    "## 5. Save Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b782b907",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b782b907",
    "outputId": "1fc42212-9ff7-420f-bafd-4d17c8bd0dcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw JSON saved to: /content/drive/MyDrive/BERTopic-arXiv-Analysis/data/raw/arxiv_cs_ai_raw.json\n",
      "Raw CSV saved to: /content/drive/MyDrive/BERTopic-arXiv-Analysis/data/raw/arxiv_cs_ai_raw.csv\n",
      "\n",
      "Total records saved: 20000\n"
     ]
    }
   ],
   "source": [
    "# Save raw data as JSON\n",
    "raw_json_path = f\"{PROJECT_PATH}/data/raw/arxiv_cs_ai_raw.json\"\n",
    "with open(raw_json_path, 'w') as f:\n",
    "    json.dump(papers, f, indent=2)\n",
    "print(f\"Raw JSON saved to: {raw_json_path}\")\n",
    "\n",
    "# Save as CSV (more convenient for pandas)\n",
    "raw_csv_path = f\"{PROJECT_PATH}/data/raw/arxiv_cs_ai_raw.csv\"\n",
    "df.to_csv(raw_csv_path, index=False)\n",
    "print(f\"Raw CSV saved to: {raw_csv_path}\")\n",
    "\n",
    "print(f\"\\nTotal records saved: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ac1019",
   "metadata": {
    "id": "74ac1019"
   },
   "source": [
    "## 6. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3fd9151",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3fd9151",
    "outputId": "1d819a04-7ad3-448b-b277-9aafeedef8c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "arxiv_id            0\n",
      "title               0\n",
      "abstract            0\n",
      "authors             0\n",
      "date                0\n",
      "year_month          0\n",
      "url                 0\n",
      "categories          0\n",
      "primary_category    0\n",
      "title_len           0\n",
      "abstract_len        0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate arxiv_ids: 0\n",
      "Short abstracts (<50 chars): 0\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "n_duplicates = df['arxiv_id'].duplicated().sum()\n",
    "print(f\"\\nDuplicate arxiv_ids: {n_duplicates}\")\n",
    "\n",
    "# Check for empty abstracts\n",
    "empty_abstracts = (df['abstract'].str.len() < 50).sum()\n",
    "print(f\"Short abstracts (<50 chars): {empty_abstracts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9410da66",
   "metadata": {
    "id": "9410da66"
   },
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has:\n",
    "1. âœ… Fetched arXiv cs.AI papers using the API\n",
    "2. âœ… Created a DataFrame with paper metadata\n",
    "3. âœ… Performed initial data exploration\n",
    "4. âœ… Saved raw data to Google Drive\n",
    "\n",
    "**Next step:** Run `02_preprocessing.ipynb` to clean and prepare the text data."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
