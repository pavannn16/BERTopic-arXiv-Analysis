{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f4a9b22",
   "metadata": {
    "id": "8f4a9b22"
   },
   "source": [
    "# Notebook 1: Data Collection\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pavannn16/BERTopic-arXiv-Analysis/blob/main/notebooks/01_data_collection.ipynb)\n",
    "\n",
    "**Purpose:** Fetch 20,000 arXiv cs.AI paper abstracts using the arXiv API.\n",
    "\n",
    "**Time:** ~15 minutes (API rate limited)\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce2bcb4e",
   "metadata": {
    "id": "ce2bcb4e"
   },
   "outputs": [],
   "source": [
    "# Install required packages (run once in Colab)\n",
    "!pip install arxiv pandas tqdm pyyaml -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52195bee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52195bee",
    "outputId": "fbbf94db-0773-4dbf-8fc0-fa79b596710f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded config from ../config.yaml\n",
      "Mode: INFER\n",
      "Running locally\n",
      "Project path: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PROJECT SETUP - Config-based with Train/Infer Modes\n",
    "# ============================================================\n",
    "import os\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Clone repo if running on Colab\n",
    "if 'google.colab' in str(get_ipython()) and not os.path.exists('/content/BERTopic-arXiv-Analysis'):\n",
    "    !git clone https://github.com/pavannn16/BERTopic-arXiv-Analysis.git /content/BERTopic-arXiv-Analysis\n",
    "\n",
    "# Load configuration\n",
    "def load_config():\n",
    "    config_paths = ['config.yaml', '../config.yaml', '/content/BERTopic-arXiv-Analysis/config.yaml']\n",
    "    for path in config_paths:\n",
    "        if os.path.exists(path):\n",
    "            with open(path, 'r') as f:\n",
    "                return yaml.safe_load(f), path\n",
    "    return None, None\n",
    "\n",
    "config, config_path = load_config()\n",
    "if config:\n",
    "    print(f\"Loaded config from {config_path}\")\n",
    "else:\n",
    "    print(\"Config not found, using defaults\")\n",
    "    config = {'mode': 'infer', 'data': {'arxiv': {'category': 'cs.AI', 'max_results': 20000}}}\n",
    "\n",
    "MODE = config.get('mode', 'infer')\n",
    "print(f\"Mode: {MODE.upper()}\")\n",
    "\n",
    "# Setup paths\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    if MODE == 'train':\n",
    "        from google.colab import drive\n",
    "        drive.mount('/content/drive')\n",
    "        PROJECT_PATH = '/content/drive/MyDrive/BERTopic-arXiv-Analysis'\n",
    "        print(\"TRAIN mode: Personal Drive mounted - will fetch fresh data\")\n",
    "    else:\n",
    "        PROJECT_PATH = '/content/BERTopic-arXiv-Analysis'\n",
    "        print(\"INFER mode: Using data from cloned repo\")\n",
    "        print(\"Data fetching skipped in INFER mode. Change mode='train' in config.yaml to fetch new data.\")\n",
    "else:\n",
    "    # Running locally\n",
    "    PROJECT_PATH = str(Path(os.getcwd()).parent) if 'notebooks' in os.getcwd() else os.getcwd()\n",
    "    print(\"Running locally\")\n",
    "\n",
    "# Create directories\n",
    "for folder in ['data/raw', 'data/processed', 'data/embeddings', 'models', 'results/visualizations']:\n",
    "    os.makedirs(f'{PROJECT_PATH}/{folder}', exist_ok=True)\n",
    "\n",
    "print(f\"Project path: {PROJECT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6990ece7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6990ece7",
    "outputId": "83618471-327f-4713-f64c-5f90509d0849"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import arxiv\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ae4f90",
   "metadata": {
    "id": "02ae4f90"
   },
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17637893",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17637893",
    "outputId": "9cd374be-bda1-42d1-e786-795a2eb7ea6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: cs.AI\n",
      "Date range: 2023-12-14 to 2025-12-03\n",
      "Max results: 20000\n",
      "\n",
      "INFER mode: Skipping data fetch. Data should already exist.\n",
      "   To fetch fresh data, set mode='train' in config.yaml\n"
     ]
    }
   ],
   "source": [
    "# Configuration - loaded from config.yaml or defaults\n",
    "ARXIV_CONFIG = config.get('data', {}).get('arxiv', {})\n",
    "\n",
    "CONFIG = {\n",
    "    'category': ARXIV_CONFIG.get('category', 'cs.AI'),\n",
    "    'max_results': ARXIV_CONFIG.get('max_results', 20000),\n",
    "    'months_back': ARXIV_CONFIG.get('months_back', 24),\n",
    "    'batch_size': ARXIV_CONFIG.get('batch_size', 100),\n",
    "    'delay_seconds': ARXIV_CONFIG.get('delay_seconds', 3.0),\n",
    "}\n",
    "\n",
    "# Calculate date range\n",
    "from datetime import datetime, timedelta\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=CONFIG['months_back'] * 30)\n",
    "\n",
    "print(f\"Category: {CONFIG['category']}\")\n",
    "print(f\"Date range: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "print(f\"Max results: {CONFIG['max_results']}\")\n",
    "\n",
    "# Check mode - skip fetching in INFER mode\n",
    "if MODE == 'infer':\n",
    "    print(\"\\nINFER mode: Skipping data fetch. Data should already exist.\")\n",
    "    print(\"   To fetch fresh data, set mode='train' in config.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32efeca5",
   "metadata": {
    "id": "32efeca5"
   },
   "source": [
    "## 3. Fetch Papers from arXiv API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "025d168b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "025d168b",
    "outputId": "3e387bf6-ee6b-44f8-f295-01832714e462"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFER mode: Skipping data fetch (data already exists in repo)\n",
      "   Loading existing data instead...\n",
      "Loaded 20000 papers from existing data\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TRAIN MODE ONLY - Skip in INFER mode\n",
    "# ============================================================\n",
    "if MODE == 'infer':\n",
    "    print(\"INFER mode: Skipping data fetch (data already exists in repo)\")\n",
    "    print(\"   Loading existing data instead...\")\n",
    "    \n",
    "    # Load existing data\n",
    "    df = pd.read_csv(f\"{PROJECT_PATH}/data/raw/arxiv_cs_ai_raw.csv\")\n",
    "    papers = df.to_dict('records')\n",
    "    print(f\"Loaded {len(papers)} papers from existing data\")\n",
    "else:\n",
    "    # TRAIN MODE - Fetch fresh data from arXiv\n",
    "    def fetch_arxiv_papers(category, max_results, batch_size=100, delay=3.0):\n",
    "        \"\"\"\n",
    "        Fetch papers from arXiv API.\n",
    "\n",
    "        Args:\n",
    "            category: arXiv category (e.g., 'cs.AI')\n",
    "            max_results: Maximum number of papers\n",
    "            batch_size: Papers per API request\n",
    "            delay: Delay between requests in seconds\n",
    "\n",
    "        Returns:\n",
    "            List of paper dictionaries\n",
    "        \"\"\"\n",
    "        query = f\"cat:{category}\"\n",
    "\n",
    "        print(f\"Fetching up to {max_results} papers from arXiv category: {category}\")\n",
    "        print(f\"This may take {max_results * delay / 60 / batch_size:.1f} minutes...\")\n",
    "\n",
    "        # Configure search\n",
    "        search = arxiv.Search(\n",
    "            query=query,\n",
    "            max_results=max_results,\n",
    "            sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "            sort_order=arxiv.SortOrder.Descending\n",
    "        )\n",
    "\n",
    "        # Configure client\n",
    "        client = arxiv.Client(\n",
    "            page_size=batch_size,\n",
    "            delay_seconds=delay,\n",
    "            num_retries=5\n",
    "        )\n",
    "\n",
    "        papers = []\n",
    "\n",
    "        try:\n",
    "            for result in tqdm(client.results(search), total=max_results, desc=\"Fetching\"):\n",
    "                paper = {\n",
    "                    \"arxiv_id\": result.entry_id.split(\"/\")[-1],\n",
    "                    \"title\": result.title.replace(\"\\n\", \" \").strip(),\n",
    "                    \"abstract\": result.summary.replace(\"\\n\", \" \").strip(),\n",
    "                    \"authors\": \", \".join([author.name for author in result.authors[:5]]),\n",
    "                    \"date\": result.published.strftime(\"%Y-%m-%d\"),\n",
    "                    \"year_month\": result.published.strftime(\"%Y-%m\"),\n",
    "                    \"url\": result.entry_id,\n",
    "                    \"categories\": \", \".join(result.categories),\n",
    "                    \"primary_category\": result.primary_category\n",
    "                }\n",
    "                papers.append(paper)\n",
    "\n",
    "                if len(papers) >= max_results:\n",
    "                    break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError during fetch: {e}\")\n",
    "            print(f\"Successfully fetched {len(papers)} papers before error\")\n",
    "\n",
    "        return papers\n",
    "\n",
    "    # Fetch papers\n",
    "    papers = fetch_arxiv_papers(\n",
    "        category=CONFIG['category'],\n",
    "        max_results=CONFIG['max_results'],\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        delay=CONFIG['delay_seconds']\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTotal papers fetched: {len(papers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a3ea92a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3a3ea92a",
    "outputId": "2de96597-89be-478a-d646-be4574a03acf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFER mode: Skipping additional fetch (using existing data)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TRAIN MODE ONLY - Fetch additional papers\n",
    "# ============================================================\n",
    "if MODE == 'infer':\n",
    "    print(\"INFER mode: Skipping additional fetch (using existing data)\")\n",
    "else:\n",
    "    # arXiv API has a 10,000 result limit per query\n",
    "    # Let's fetch additional papers by querying different date ranges\n",
    "\n",
    "    def fetch_arxiv_by_date_range(category, start_date, end_date, max_results=10000, batch_size=100, delay=3.0):\n",
    "        \"\"\"Fetch papers within a specific date range.\"\"\"\n",
    "        start_str = start_date.strftime(\"%Y%m%d\")\n",
    "        end_str = end_date.strftime(\"%Y%m%d\")\n",
    "\n",
    "        query = f\"cat:{category} AND submittedDate:[{start_str}0000 TO {end_str}2359]\"\n",
    "\n",
    "        print(f\"Fetching papers from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "        search = arxiv.Search(\n",
    "            query=query,\n",
    "            max_results=max_results,\n",
    "            sort_by=arxiv.SortCriterion.SubmittedDate,\n",
    "            sort_order=arxiv.SortOrder.Descending\n",
    "        )\n",
    "\n",
    "        client = arxiv.Client(\n",
    "            page_size=batch_size,\n",
    "            delay_seconds=delay,\n",
    "            num_retries=5\n",
    "        )\n",
    "\n",
    "        papers = []\n",
    "\n",
    "        try:\n",
    "            for result in tqdm(client.results(search), total=max_results, desc=\"Fetching\"):\n",
    "                paper = {\n",
    "                    \"arxiv_id\": result.entry_id.split(\"/\")[-1],\n",
    "                    \"title\": result.title.replace(\"\\n\", \" \").strip(),\n",
    "                    \"abstract\": result.summary.replace(\"\\n\", \" \").strip(),\n",
    "                    \"authors\": \", \".join([author.name for author in result.authors[:5]]),\n",
    "                    \"date\": result.published.strftime(\"%Y-%m-%d\"),\n",
    "                    \"year_month\": result.published.strftime(\"%Y-%m\"),\n",
    "                    \"url\": result.entry_id,\n",
    "                    \"categories\": \", \".join(result.categories),\n",
    "                    \"primary_category\": result.primary_category\n",
    "                }\n",
    "                papers.append(paper)\n",
    "\n",
    "                if len(papers) >= max_results:\n",
    "                    break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {e}\")\n",
    "            print(f\"Fetched {len(papers)} papers before error\")\n",
    "\n",
    "        return papers\n",
    "\n",
    "    # Find the earliest date we have\n",
    "    earliest_date = pd.to_datetime(min([p['date'] for p in papers]))\n",
    "    print(f\"Earliest paper in current batch: {earliest_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "    # Fetch earlier papers\n",
    "    end_date_batch2 = earliest_date - timedelta(days=1)\n",
    "    start_date_batch2 = earliest_date - timedelta(days=180)\n",
    "\n",
    "    print(f\"\\nFetching additional papers from {start_date_batch2.strftime('%Y-%m-%d')} to {end_date_batch2.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "    papers_batch2 = fetch_arxiv_by_date_range(\n",
    "        category=CONFIG['category'],\n",
    "        start_date=start_date_batch2,\n",
    "        end_date=end_date_batch2,\n",
    "        max_results=10000,\n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        delay=CONFIG['delay_seconds']\n",
    "    )\n",
    "\n",
    "    print(f\"\\nAdditional papers fetched: {len(papers_batch2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97f5a0f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97f5a0f5",
    "outputId": "3116de1b-50a1-4e4f-f512-842d13757017"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFER mode: Using existing data (already loaded)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# TRAIN MODE ONLY - Combine batches and deduplicate\n",
    "# ============================================================\n",
    "if MODE == 'infer':\n",
    "    print(\"INFER mode: Using existing data (already loaded)\")\n",
    "else:\n",
    "    # Check what we have so far\n",
    "    print(f\"Batch 1 (recent): {len(papers)} papers\")\n",
    "    print(f\"Batch 2 (earlier): {len(papers_batch2)} papers\")\n",
    "\n",
    "    # Combine and deduplicate\n",
    "    all_papers = papers + papers_batch2\n",
    "    seen_ids = set()\n",
    "    unique_papers = []\n",
    "    for p in all_papers:\n",
    "        if p['arxiv_id'] not in seen_ids:\n",
    "            seen_ids.add(p['arxiv_id'])\n",
    "            unique_papers.append(p)\n",
    "\n",
    "    print(f\"Total unique papers: {len(unique_papers)}\")\n",
    "\n",
    "    # Check date range\n",
    "    dates = [p['date'] for p in unique_papers]\n",
    "    print(f\"Date range: {min(dates)} to {max(dates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ad8d877",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ad8d877",
    "outputId": "c7fb68ee-6678-4df5-bb6d-96acaa610f7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFER mode: Loaded 20,000 papers\n",
      "ðŸ“… Date range: 2025-07-03 to 2025-12-02\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Finalize papers dataset\n",
    "# ============================================================\n",
    "if MODE == 'infer':\n",
    "    # Already loaded papers from CSV in cell 8\n",
    "    df = pd.read_csv(f\"{PROJECT_PATH}/data/raw/arxiv_cs_ai_raw.csv\")\n",
    "    papers = df.to_dict('records')\n",
    "    dates = [p['date'] for p in papers]\n",
    "    print(f\"INFER mode: Loaded {len(papers):,} papers\")\n",
    "    print(f\"ðŸ“… Date range: {min(dates)} to {max(dates)}\")\n",
    "else:\n",
    "    # Use the combined unique papers\n",
    "    papers = unique_papers\n",
    "    print(f\"Final dataset: {len(papers)} papers\")\n",
    "    print(f\"ðŸ“… Date range: {min(dates)} to {max(dates)} (recent 5 months)\")\n",
    "    print(f\"ðŸŽ¯ Target achieved: 20,000 recent cs.AI papers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a123fc52",
   "metadata": {
    "id": "a123fc52"
   },
   "source": [
    "## 4. Create DataFrame and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "618d8755",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 504
    },
    "id": "618d8755",
    "outputId": "afb02553-048e-4f6a-e098-7a17a25d23e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape: (20000, 11)\n",
      "\n",
      "Columns: ['arxiv_id', 'title', 'abstract', 'authors', 'date', 'year_month', 'url', 'categories', 'primary_category', 'title_len', 'abstract_len']\n",
      "\n",
      "Date range: 2025-07-03 to 2025-12-02\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arxiv_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "      <th>year_month</th>\n",
       "      <th>url</th>\n",
       "      <th>categories</th>\n",
       "      <th>primary_category</th>\n",
       "      <th>title_len</th>\n",
       "      <th>abstract_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2512.02987v1</td>\n",
       "      <td>Fine-Tuned Large Language Models for Logical T...</td>\n",
       "      <td>Recent advances in natural language processing...</td>\n",
       "      <td>Muyu Pan, Dheeraj Kodakandla, Mahfuza Farooque</td>\n",
       "      <td>2025-12-02</td>\n",
       "      <td>2025-12</td>\n",
       "      <td>http://arxiv.org/abs/2512.02987v1</td>\n",
       "      <td>cs.CL, cs.AI</td>\n",
       "      <td>cs.CL</td>\n",
       "      <td>97</td>\n",
       "      <td>1079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2512.02978v1</td>\n",
       "      <td>Rethinking Generalized BCIs: Benchmarking 340,...</td>\n",
       "      <td>Robust decoding and classification of brain pa...</td>\n",
       "      <td>Paul Barbaste, Olivier Oullier, Xavier Vasques</td>\n",
       "      <td>2025-12-02</td>\n",
       "      <td>2025-12</td>\n",
       "      <td>http://arxiv.org/abs/2512.02978v1</td>\n",
       "      <td>q-bio.NC, cs.AI, cs.HC, cs.LG</td>\n",
       "      <td>q-bio.NC</td>\n",
       "      <td>116</td>\n",
       "      <td>1674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2512.02966v1</td>\n",
       "      <td>Lumos: Let there be Language Model System Cert...</td>\n",
       "      <td>We introduce the first principled framework, L...</td>\n",
       "      <td>Isha Chaudhary, Vedaant Jain, Avaljot Singh, K...</td>\n",
       "      <td>2025-12-02</td>\n",
       "      <td>2025-12</td>\n",
       "      <td>http://arxiv.org/abs/2512.02966v1</td>\n",
       "      <td>cs.PL, cs.AI, cs.MA</td>\n",
       "      <td>cs.PL</td>\n",
       "      <td>55</td>\n",
       "      <td>1693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2512.02942v1</td>\n",
       "      <td>Benchmarking Scientific Understanding and Reas...</td>\n",
       "      <td>The next frontier for video generation lies in...</td>\n",
       "      <td>Lanxiang Hu, Abhilash Shankarampeta, Yixin Hua...</td>\n",
       "      <td>2025-12-02</td>\n",
       "      <td>2025-12</td>\n",
       "      <td>http://arxiv.org/abs/2512.02942v1</td>\n",
       "      <td>cs.CV, cs.AI</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>97</td>\n",
       "      <td>1519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2512.02932v1</td>\n",
       "      <td>EGGS: Exchangeable 2D/3D Gaussian Splatting fo...</td>\n",
       "      <td>Novel view synthesis (NVS) is crucial in compu...</td>\n",
       "      <td>Yancheng Zhang, Guangyu Sun, Chen Chen</td>\n",
       "      <td>2025-12-02</td>\n",
       "      <td>2025-12</td>\n",
       "      <td>http://arxiv.org/abs/2512.02932v1</td>\n",
       "      <td>cs.CV, cs.AI</td>\n",
       "      <td>cs.CV</td>\n",
       "      <td>97</td>\n",
       "      <td>1117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       arxiv_id                                              title  \\\n",
       "0  2512.02987v1  Fine-Tuned Large Language Models for Logical T...   \n",
       "1  2512.02978v1  Rethinking Generalized BCIs: Benchmarking 340,...   \n",
       "2  2512.02966v1  Lumos: Let there be Language Model System Cert...   \n",
       "3  2512.02942v1  Benchmarking Scientific Understanding and Reas...   \n",
       "4  2512.02932v1  EGGS: Exchangeable 2D/3D Gaussian Splatting fo...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Recent advances in natural language processing...   \n",
       "1  Robust decoding and classification of brain pa...   \n",
       "2  We introduce the first principled framework, L...   \n",
       "3  The next frontier for video generation lies in...   \n",
       "4  Novel view synthesis (NVS) is crucial in compu...   \n",
       "\n",
       "                                             authors        date year_month  \\\n",
       "0     Muyu Pan, Dheeraj Kodakandla, Mahfuza Farooque  2025-12-02    2025-12   \n",
       "1     Paul Barbaste, Olivier Oullier, Xavier Vasques  2025-12-02    2025-12   \n",
       "2  Isha Chaudhary, Vedaant Jain, Avaljot Singh, K...  2025-12-02    2025-12   \n",
       "3  Lanxiang Hu, Abhilash Shankarampeta, Yixin Hua...  2025-12-02    2025-12   \n",
       "4             Yancheng Zhang, Guangyu Sun, Chen Chen  2025-12-02    2025-12   \n",
       "\n",
       "                                 url                     categories  \\\n",
       "0  http://arxiv.org/abs/2512.02987v1                   cs.CL, cs.AI   \n",
       "1  http://arxiv.org/abs/2512.02978v1  q-bio.NC, cs.AI, cs.HC, cs.LG   \n",
       "2  http://arxiv.org/abs/2512.02966v1            cs.PL, cs.AI, cs.MA   \n",
       "3  http://arxiv.org/abs/2512.02942v1                   cs.CV, cs.AI   \n",
       "4  http://arxiv.org/abs/2512.02932v1                   cs.CV, cs.AI   \n",
       "\n",
       "  primary_category  title_len  abstract_len  \n",
       "0            cs.CL         97          1079  \n",
       "1         q-bio.NC        116          1674  \n",
       "2            cs.PL         55          1693  \n",
       "3            cs.CV         97          1519  \n",
       "4            cs.CV         97          1117  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame(papers)\n",
    "\n",
    "print(f\"DataFrame shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nDate range: {df['date'].min()} to {df['date'].max()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8957fb8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d8957fb8",
    "outputId": "a7a25412-949b-4109-f382-728fc1c42f4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Statistics:\n",
      "  Total papers: 20000\n",
      "  Unique dates: 153\n",
      "  Date range: 2025-07-03 to 2025-12-02\n",
      "\n",
      "Title length: mean=83, median=83\n",
      "Abstract length: mean=1340, median=1340\n"
     ]
    }
   ],
   "source": [
    "# Basic statistics\n",
    "print(\"Dataset Statistics:\")\n",
    "print(f\"  Total papers: {len(df)}\")\n",
    "print(f\"  Unique dates: {df['date'].nunique()}\")\n",
    "print(f\"  Date range: {df['date'].min()} to {df['date'].max()}\")\n",
    "\n",
    "# Text length statistics\n",
    "df['title_len'] = df['title'].str.len()\n",
    "df['abstract_len'] = df['abstract'].str.len()\n",
    "\n",
    "print(f\"\\nTitle length: mean={df['title_len'].mean():.0f}, median={df['title_len'].median():.0f}\")\n",
    "print(f\"Abstract length: mean={df['abstract_len'].mean():.0f}, median={df['abstract_len'].median():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33536339",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33536339",
    "outputId": "91e6cd90-df2a-48f1-d5aa-5062e6971068"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers per month:\n",
      "year_month\n",
      "2025-07    3156\n",
      "2025-08    3819\n",
      "2025-09    4215\n",
      "2025-10    4821\n",
      "2025-11    3755\n",
      "2025-12     234\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Papers per month\n",
    "papers_per_month = df['year_month'].value_counts().sort_index()\n",
    "print(\"Papers per month:\")\n",
    "print(papers_per_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b160792c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b160792c",
    "outputId": "4014e4c5-1844-42dd-e48b-b92e56747d6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample abstracts:\n",
      "================================================================================\n",
      "\n",
      "Title: Fine-Tuned Large Language Models for Logical Translation: Reducing Hallucinations with Lang2Logic\n",
      "Date: 2025-12-02\n",
      "Abstract: Recent advances in natural language processing (NLP), particularly large language models (LLMs), have motivated the automatic translation of natural language statements into formal logic without human intervention. This enables automated reasoning and facilitates debugging, finding loop invariants, ...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Rethinking Generalized BCIs: Benchmarking 340,000+ Unique Algorithmic Configurations for EEG Mental Command Decoding\n",
      "Date: 2025-12-02\n",
      "Abstract: Robust decoding and classification of brain patterns measured with electroencephalography (EEG) remains a major challenge for real-world (i.e. outside scientific lab and medical facilities) brain-computer interface (BCI) applications due to well documented inter- and intra-participant variability. H...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Title: Lumos: Let there be Language Model System Certification\n",
      "Date: 2025-12-02\n",
      "Abstract: We introduce the first principled framework, Lumos, for specifying and formally certifying Language Model System (LMS) behaviors. Lumos is an imperative probabilistic programming DSL over graphs, with constructs to generate independent and identically distributed prompts for LMS. It offers a structu...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Sample abstracts\n",
    "print(\"Sample abstracts:\")\n",
    "print(\"=\" * 80)\n",
    "for i, row in df.head(3).iterrows():\n",
    "    print(f\"\\nTitle: {row['title']}\")\n",
    "    print(f\"Date: {row['date']}\")\n",
    "    print(f\"Abstract: {row['abstract'][:300]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a0bddb",
   "metadata": {
    "id": "34a0bddb"
   },
   "source": [
    "## 5. Save Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b782b907",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b782b907",
    "outputId": "1fc42212-9ff7-420f-bafd-4d17c8bd0dcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFER mode: Skipping save (data already exists)\n",
      "Existing data at: /Users/pavan/Downloads/CSULA SEM1/AI/Code Assignments/BERTopic-arXiv-Analysis/data/raw/arxiv_cs_ai_raw.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Save raw data (TRAIN mode only)\n",
    "# ============================================================\n",
    "if MODE == 'infer':\n",
    "    print(\"INFER mode: Skipping save (data already exists)\")\n",
    "    print(f\"Existing data at: {PROJECT_PATH}/data/raw/arxiv_cs_ai_raw.csv\")\n",
    "else:\n",
    "    # Save raw data as JSON\n",
    "    raw_json_path = f\"{PROJECT_PATH}/data/raw/arxiv_cs_ai_raw.json\"\n",
    "    with open(raw_json_path, 'w') as f:\n",
    "        json.dump(papers, f, indent=2)\n",
    "    print(f\"Raw JSON saved to: {raw_json_path}\")\n",
    "\n",
    "    # Save as CSV (more convenient for pandas)\n",
    "    raw_csv_path = f\"{PROJECT_PATH}/data/raw/arxiv_cs_ai_raw.csv\"\n",
    "    df.to_csv(raw_csv_path, index=False)\n",
    "    print(f\"Raw CSV saved to: {raw_csv_path}\")\n",
    "\n",
    "    print(f\"\\nTotal records saved: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ac1019",
   "metadata": {
    "id": "74ac1019"
   },
   "source": [
    "## 6. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3fd9151",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3fd9151",
    "outputId": "1d819a04-7ad3-448b-b277-9aafeedef8c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values:\n",
      "arxiv_id            0\n",
      "title               0\n",
      "abstract            0\n",
      "authors             0\n",
      "date                0\n",
      "year_month          0\n",
      "url                 0\n",
      "categories          0\n",
      "primary_category    0\n",
      "title_len           0\n",
      "abstract_len        0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate arxiv_ids: 0\n",
      "Short abstracts (<50 chars): 0\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for duplicates\n",
    "n_duplicates = df['arxiv_id'].duplicated().sum()\n",
    "print(f\"\\nDuplicate arxiv_ids: {n_duplicates}\")\n",
    "\n",
    "# Check for empty abstracts\n",
    "empty_abstracts = (df['abstract'].str.len() < 50).sum()\n",
    "print(f\"Short abstracts (<50 chars): {empty_abstracts}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
