{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8f4a9b22",
      "metadata": {
        "id": "8f4a9b22"
      },
      "source": [
        "# üìä Notebook 1: Data Collection\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pavannn16/BERTopic-arXiv-Analysis/blob/main/notebooks/01_data_collection.ipynb)\n",
        "\n",
        "**Purpose:** Fetch 20,000 arXiv cs.AI paper abstracts using the arXiv API.\n",
        "\n",
        "**Time:** ~15 minutes (API rate limited)\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ce2bcb4e",
      "metadata": {
        "id": "ce2bcb4e"
      },
      "outputs": [],
      "source": [
        "# Install required packages (run once in Colab)\n",
        "!pip install arxiv pandas tqdm -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "52195bee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52195bee",
        "outputId": "fbbf94db-0773-4dbf-8fc0-fa79b596710f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "‚úÖ Running on Google Colab\n",
            "üìÅ Project path: /content/drive/MyDrive/BERTopic-arXiv-Analysis\n",
            "üìÇ Directories ready:\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# PROJECT PATH SETUP - Works on Colab Web, VS Code, or Local\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Detect environment and set project path\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    # Running on Google Colab - mount Drive\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    PROJECT_PATH = '/content/drive/MyDrive/BERTopic-arXiv-Analysis'\n",
        "    print(\"‚úÖ Running on Google Colab\")\n",
        "else:\n",
        "    # Running locally (VS Code, Jupyter, etc.)\n",
        "    PROJECT_PATH = str(Path(os.getcwd()).parent) if 'notebooks' in os.getcwd() else os.getcwd()\n",
        "    print(\"‚úÖ Running locally\")\n",
        "\n",
        "# Create directory structure\n",
        "for folder in ['data/raw', 'data/processed', 'data/embeddings', 'models', 'results/visualizations']:\n",
        "    os.makedirs(f'{PROJECT_PATH}/{folder}', exist_ok=True)\n",
        "\n",
        "print(f\"üìÅ Project path: {PROJECT_PATH}\")\n",
        "print(\"üìÇ Directories ready:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "6990ece7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6990ece7",
        "outputId": "83618471-327f-4713-f64c-5f90509d0849"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import arxiv\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import time\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02ae4f90",
      "metadata": {
        "id": "02ae4f90"
      },
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "17637893",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17637893",
        "outputId": "9cd374be-bda1-42d1-e786-795a2eb7ea6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Category: cs.AI\n",
            "Date range: 2023-12-14 to 2025-12-03\n",
            "Max results: 20000\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "CONFIG = {\n",
        "    'category': 'cs.AI',           # arXiv category\n",
        "    'max_results': 20000,          # Maximum papers to fetch (full scope)\n",
        "    'months_back': 24,             # How many months of data (2 years)\n",
        "    'batch_size': 100,             # Papers per API request\n",
        "    'delay_seconds': 3.0,          # Delay between requests (be respectful)\n",
        "}\n",
        "\n",
        "# Calculate date range\n",
        "end_date = datetime.now()\n",
        "start_date = end_date - timedelta(days=CONFIG['months_back'] * 30)\n",
        "\n",
        "print(f\"Category: {CONFIG['category']}\")\n",
        "print(f\"Date range: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
        "print(f\"Max results: {CONFIG['max_results']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32efeca5",
      "metadata": {
        "id": "32efeca5"
      },
      "source": [
        "## 3. Fetch Papers from arXiv API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "025d168b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "025d168b",
        "outputId": "3e387bf6-ee6b-44f8-f295-01832714e462"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching up to 20000 papers from arXiv category: cs.AI\n",
            "This may take 10.0 minutes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 10000/20000 [06:55<06:55, 24.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Error during fetch: Page request resulted in HTTP 500 (https://export.arxiv.org/api/query?search_query=cat%3Acs.AI&id_list=&sortBy=submittedDate&sortOrder=descending&start=10000&max_results=100)\n",
            "Successfully fetched 10000 papers before error\n",
            "\n",
            "Total papers fetched: 10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def fetch_arxiv_papers(category, max_results, batch_size=100, delay=3.0):\n",
        "    \"\"\"\n",
        "    Fetch papers from arXiv API.\n",
        "\n",
        "    Args:\n",
        "        category: arXiv category (e.g., 'cs.AI')\n",
        "        max_results: Maximum number of papers\n",
        "        batch_size: Papers per API request\n",
        "        delay: Delay between requests in seconds\n",
        "\n",
        "    Returns:\n",
        "        List of paper dictionaries\n",
        "    \"\"\"\n",
        "    query = f\"cat:{category}\"\n",
        "\n",
        "    print(f\"Fetching up to {max_results} papers from arXiv category: {category}\")\n",
        "    print(f\"This may take {max_results * delay / 60 / batch_size:.1f} minutes...\")\n",
        "\n",
        "    # Configure search\n",
        "    search = arxiv.Search(\n",
        "        query=query,\n",
        "        max_results=max_results,\n",
        "        sort_by=arxiv.SortCriterion.SubmittedDate,\n",
        "        sort_order=arxiv.SortOrder.Descending\n",
        "    )\n",
        "\n",
        "    # Configure client\n",
        "    client = arxiv.Client(\n",
        "        page_size=batch_size,\n",
        "        delay_seconds=delay,\n",
        "        num_retries=5\n",
        "    )\n",
        "\n",
        "    papers = []\n",
        "\n",
        "    try:\n",
        "        for result in tqdm(client.results(search), total=max_results, desc=\"Fetching\"):\n",
        "            paper = {\n",
        "                \"arxiv_id\": result.entry_id.split(\"/\")[-1],\n",
        "                \"title\": result.title.replace(\"\\n\", \" \").strip(),\n",
        "                \"abstract\": result.summary.replace(\"\\n\", \" \").strip(),\n",
        "                \"authors\": \", \".join([author.name for author in result.authors[:5]]),  # First 5 authors\n",
        "                \"date\": result.published.strftime(\"%Y-%m-%d\"),\n",
        "                \"year_month\": result.published.strftime(\"%Y-%m\"),\n",
        "                \"url\": result.entry_id,\n",
        "                \"categories\": \", \".join(result.categories),\n",
        "                \"primary_category\": result.primary_category\n",
        "            }\n",
        "            papers.append(paper)\n",
        "\n",
        "            if len(papers) >= max_results:\n",
        "                break\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError during fetch: {e}\")\n",
        "        print(f\"Successfully fetched {len(papers)} papers before error\")\n",
        "\n",
        "    return papers\n",
        "\n",
        "# Fetch papers\n",
        "papers = fetch_arxiv_papers(\n",
        "    category=CONFIG['category'],\n",
        "    max_results=CONFIG['max_results'],\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    delay=CONFIG['delay_seconds']\n",
        ")\n",
        "\n",
        "print(f\"\\nTotal papers fetched: {len(papers)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3a3ea92a",
      "metadata": {
        "id": "3a3ea92a",
        "outputId": "2de96597-89be-478a-d646-be4574a03acf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Earliest paper in current batch: 2025-09-26\n",
            "\n",
            "Fetching additional papers from 2025-03-30 to 2025-09-25\n",
            "Fetching papers from 2025-03-30 to 2025-09-25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 9999/10000 [05:36<00:00, 29.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Additional papers fetched: 10000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# arXiv API has a 10,000 result limit per query\n",
        "# Let's fetch additional papers by querying different date ranges\n",
        "# We already have 10,000 - let's get more from an earlier period\n",
        "\n",
        "def fetch_arxiv_by_date_range(category, start_date, end_date, max_results=10000, batch_size=100, delay=3.0):\n",
        "    \"\"\"Fetch papers within a specific date range.\"\"\"\n",
        "    # Format dates for arXiv query\n",
        "    start_str = start_date.strftime(\"%Y%m%d\")\n",
        "    end_str = end_date.strftime(\"%Y%m%d\")\n",
        "\n",
        "    query = f\"cat:{category} AND submittedDate:[{start_str}0000 TO {end_str}2359]\"\n",
        "\n",
        "    print(f\"Fetching papers from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "    search = arxiv.Search(\n",
        "        query=query,\n",
        "        max_results=max_results,\n",
        "        sort_by=arxiv.SortCriterion.SubmittedDate,\n",
        "        sort_order=arxiv.SortOrder.Descending\n",
        "    )\n",
        "\n",
        "    client = arxiv.Client(\n",
        "        page_size=batch_size,\n",
        "        delay_seconds=delay,\n",
        "        num_retries=5\n",
        "    )\n",
        "\n",
        "    papers = []\n",
        "\n",
        "    try:\n",
        "        for result in tqdm(client.results(search), total=max_results, desc=\"Fetching\"):\n",
        "            paper = {\n",
        "                \"arxiv_id\": result.entry_id.split(\"/\")[-1],\n",
        "                \"title\": result.title.replace(\"\\n\", \" \").strip(),\n",
        "                \"abstract\": result.summary.replace(\"\\n\", \" \").strip(),\n",
        "                \"authors\": \", \".join([author.name for author in result.authors[:5]]),\n",
        "                \"date\": result.published.strftime(\"%Y-%m-%d\"),\n",
        "                \"year_month\": result.published.strftime(\"%Y-%m\"),\n",
        "                \"url\": result.entry_id,\n",
        "                \"categories\": \", \".join(result.categories),\n",
        "                \"primary_category\": result.primary_category\n",
        "            }\n",
        "            papers.append(paper)\n",
        "\n",
        "            if len(papers) >= max_results:\n",
        "                break\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError: {e}\")\n",
        "        print(f\"Fetched {len(papers)} papers before error\")\n",
        "\n",
        "    return papers\n",
        "\n",
        "# Find the earliest date we have\n",
        "earliest_date = pd.to_datetime(min([p['date'] for p in papers]))\n",
        "print(f\"Earliest paper in current batch: {earliest_date.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "# Fetch earlier papers (before our current earliest date)\n",
        "end_date_batch2 = earliest_date - timedelta(days=1)\n",
        "start_date_batch2 = earliest_date - timedelta(days=180)  # 6 months earlier\n",
        "\n",
        "print(f\"\\nFetching additional papers from {start_date_batch2.strftime('%Y-%m-%d')} to {end_date_batch2.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "papers_batch2 = fetch_arxiv_by_date_range(\n",
        "    category=CONFIG['category'],\n",
        "    start_date=start_date_batch2,\n",
        "    end_date=end_date_batch2,\n",
        "    max_results=10000,\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    delay=CONFIG['delay_seconds']\n",
        ")\n",
        "\n",
        "print(f\"\\nAdditional papers fetched: {len(papers_batch2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "97f5a0f5",
      "metadata": {
        "id": "97f5a0f5",
        "outputId": "3116de1b-50a1-4e4f-f512-842d13757017",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1 (recent): 10000 papers\n",
            "Batch 2 (earlier): 10000 papers\n",
            "Total unique papers: 20000\n",
            "Date range: 2025-07-03 to 2025-12-02\n"
          ]
        }
      ],
      "source": [
        "# Check what we have so far\n",
        "print(f\"Batch 1 (recent): {len(papers)} papers\")\n",
        "print(f\"Batch 2 (earlier): {len(papers_batch2)} papers\")\n",
        "\n",
        "# Combine and deduplicate\n",
        "all_papers = papers + papers_batch2\n",
        "seen_ids = set()\n",
        "unique_papers = []\n",
        "for p in all_papers:\n",
        "    if p['arxiv_id'] not in seen_ids:\n",
        "        seen_ids.add(p['arxiv_id'])\n",
        "        unique_papers.append(p)\n",
        "\n",
        "print(f\"Total unique papers: {len(unique_papers)}\")\n",
        "\n",
        "# Check date range\n",
        "dates = [p['date'] for p in unique_papers]\n",
        "print(f\"Date range: {min(dates)} to {max(dates)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "3ad8d877",
      "metadata": {
        "id": "3ad8d877",
        "outputId": "c7fb68ee-6678-4df5-bb6d-96acaa610f7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Final dataset: 20000 papers\n",
            "üìÖ Date range: 2025-07-03 to 2025-12-02 (recent 5 months)\n",
            "üéØ Target achieved: 20,000 recent cs.AI papers!\n"
          ]
        }
      ],
      "source": [
        "# Use the combined unique papers\n",
        "papers = unique_papers\n",
        "print(f\"‚úÖ Final dataset: {len(papers)} papers\")\n",
        "print(f\"üìÖ Date range: {min(dates)} to {max(dates)} (recent 5 months)\")\n",
        "print(f\"üéØ Target achieved: 20,000 recent cs.AI papers!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a123fc52",
      "metadata": {
        "id": "a123fc52"
      },
      "source": [
        "## 4. Create DataFrame and Explore Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "618d8755",
      "metadata": {
        "id": "618d8755",
        "outputId": "afb02553-048e-4f6a-e098-7a17a25d23e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame shape: (20000, 9)\n",
            "\n",
            "Columns: ['arxiv_id', 'title', 'abstract', 'authors', 'date', 'year_month', 'url', 'categories', 'primary_category']\n",
            "\n",
            "Date range: 2025-07-03 to 2025-12-02\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       arxiv_id                                              title  \\\n",
              "0  2512.02987v1  Fine-Tuned Large Language Models for Logical T...   \n",
              "1  2512.02978v1  Rethinking Generalized BCIs: Benchmarking 340,...   \n",
              "2  2512.02966v1  Lumos: Let there be Language Model System Cert...   \n",
              "3  2512.02942v1  Benchmarking Scientific Understanding and Reas...   \n",
              "4  2512.02932v1  EGGS: Exchangeable 2D/3D Gaussian Splatting fo...   \n",
              "\n",
              "                                            abstract  \\\n",
              "0  Recent advances in natural language processing...   \n",
              "1  Robust decoding and classification of brain pa...   \n",
              "2  We introduce the first principled framework, L...   \n",
              "3  The next frontier for video generation lies in...   \n",
              "4  Novel view synthesis (NVS) is crucial in compu...   \n",
              "\n",
              "                                             authors        date year_month  \\\n",
              "0     Muyu Pan, Dheeraj Kodakandla, Mahfuza Farooque  2025-12-02    2025-12   \n",
              "1     Paul Barbaste, Olivier Oullier, Xavier Vasques  2025-12-02    2025-12   \n",
              "2  Isha Chaudhary, Vedaant Jain, Avaljot Singh, K...  2025-12-02    2025-12   \n",
              "3  Lanxiang Hu, Abhilash Shankarampeta, Yixin Hua...  2025-12-02    2025-12   \n",
              "4             Yancheng Zhang, Guangyu Sun, Chen Chen  2025-12-02    2025-12   \n",
              "\n",
              "                                 url                     categories  \\\n",
              "0  http://arxiv.org/abs/2512.02987v1                   cs.CL, cs.AI   \n",
              "1  http://arxiv.org/abs/2512.02978v1  q-bio.NC, cs.AI, cs.HC, cs.LG   \n",
              "2  http://arxiv.org/abs/2512.02966v1            cs.PL, cs.AI, cs.MA   \n",
              "3  http://arxiv.org/abs/2512.02942v1                   cs.CV, cs.AI   \n",
              "4  http://arxiv.org/abs/2512.02932v1                   cs.CV, cs.AI   \n",
              "\n",
              "  primary_category  \n",
              "0            cs.CL  \n",
              "1         q-bio.NC  \n",
              "2            cs.PL  \n",
              "3            cs.CV  \n",
              "4            cs.CV  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-119631eb-d093-4da0-b457-10f63642dd7d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>arxiv_id</th>\n",
              "      <th>title</th>\n",
              "      <th>abstract</th>\n",
              "      <th>authors</th>\n",
              "      <th>date</th>\n",
              "      <th>year_month</th>\n",
              "      <th>url</th>\n",
              "      <th>categories</th>\n",
              "      <th>primary_category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2512.02987v1</td>\n",
              "      <td>Fine-Tuned Large Language Models for Logical T...</td>\n",
              "      <td>Recent advances in natural language processing...</td>\n",
              "      <td>Muyu Pan, Dheeraj Kodakandla, Mahfuza Farooque</td>\n",
              "      <td>2025-12-02</td>\n",
              "      <td>2025-12</td>\n",
              "      <td>http://arxiv.org/abs/2512.02987v1</td>\n",
              "      <td>cs.CL, cs.AI</td>\n",
              "      <td>cs.CL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2512.02978v1</td>\n",
              "      <td>Rethinking Generalized BCIs: Benchmarking 340,...</td>\n",
              "      <td>Robust decoding and classification of brain pa...</td>\n",
              "      <td>Paul Barbaste, Olivier Oullier, Xavier Vasques</td>\n",
              "      <td>2025-12-02</td>\n",
              "      <td>2025-12</td>\n",
              "      <td>http://arxiv.org/abs/2512.02978v1</td>\n",
              "      <td>q-bio.NC, cs.AI, cs.HC, cs.LG</td>\n",
              "      <td>q-bio.NC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2512.02966v1</td>\n",
              "      <td>Lumos: Let there be Language Model System Cert...</td>\n",
              "      <td>We introduce the first principled framework, L...</td>\n",
              "      <td>Isha Chaudhary, Vedaant Jain, Avaljot Singh, K...</td>\n",
              "      <td>2025-12-02</td>\n",
              "      <td>2025-12</td>\n",
              "      <td>http://arxiv.org/abs/2512.02966v1</td>\n",
              "      <td>cs.PL, cs.AI, cs.MA</td>\n",
              "      <td>cs.PL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2512.02942v1</td>\n",
              "      <td>Benchmarking Scientific Understanding and Reas...</td>\n",
              "      <td>The next frontier for video generation lies in...</td>\n",
              "      <td>Lanxiang Hu, Abhilash Shankarampeta, Yixin Hua...</td>\n",
              "      <td>2025-12-02</td>\n",
              "      <td>2025-12</td>\n",
              "      <td>http://arxiv.org/abs/2512.02942v1</td>\n",
              "      <td>cs.CV, cs.AI</td>\n",
              "      <td>cs.CV</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2512.02932v1</td>\n",
              "      <td>EGGS: Exchangeable 2D/3D Gaussian Splatting fo...</td>\n",
              "      <td>Novel view synthesis (NVS) is crucial in compu...</td>\n",
              "      <td>Yancheng Zhang, Guangyu Sun, Chen Chen</td>\n",
              "      <td>2025-12-02</td>\n",
              "      <td>2025-12</td>\n",
              "      <td>http://arxiv.org/abs/2512.02932v1</td>\n",
              "      <td>cs.CV, cs.AI</td>\n",
              "      <td>cs.CV</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-119631eb-d093-4da0-b457-10f63642dd7d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-119631eb-d093-4da0-b457-10f63642dd7d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-119631eb-d093-4da0-b457-10f63642dd7d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-88edab04-02a7-45dc-a604-3df9621223e4\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-88edab04-02a7-45dc-a604-3df9621223e4')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-88edab04-02a7-45dc-a604-3df9621223e4 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 20000,\n  \"fields\": [\n    {\n      \"column\": \"arxiv_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20000,\n        \"samples\": [\n          \"2509.20386v1\",\n          \"2511.12449v1\",\n          \"2510.01069v1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 19999,\n        \"samples\": [\n          \"Dynamic ReAct: Scalable Tool Selection for Large-Scale MCP Environments\",\n          \"MOON2.0: Dynamic Modality-balanced Multimodal Representation Learning for E-commerce Product Understanding\",\n          \"Typed Chain-of-Thought: A Curry-Howard Framework for Verifying LLM Reasoning\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 19997,\n        \"samples\": [\n          \"This paper introduces MCTS-EP, an online learning framework that combines large language models (LLM) with Monte Carlo Tree Search (MCTS) for training embodied agents. MCTS-EP integrates three key components: MCTS-guided exploration for preference data collection, efficient multi-modal reasoning mechanism, and iterative training pipeline based on preference optimization. We theoretically prove that MCTS-EP achieves better performance bounds than conventional on-policy algorithms when the loss function is strongly convex, and demonstrate that it can be formulated as a search-enhanced variant of GAIL. MCTS-EP achieves state-of-the-art performace across serval benchmarks. In ALFWorld, it achieves 92% and 87% success rates for textual and visual tasks. In WebShop, it reaches an average reward of 0.81. MTCS-EP also reduces average interaction steps from from 18.7/19.5 to 10.2/9.9 steps in visual ALFWorld.Code available at: https://github.com/xuhang-2/Embodied-Agent-Planning\",\n          \"The integration of Artificial Intelligence (AI) into safety-critical systems introduces a new reliability paradigm: silent failures, where AI produces confident but incorrect outputs that can be dangerous. This paper introduces the Formal Assurance and Monitoring Environment (FAME), a novel framework that confronts this challenge. FAME synergizes the mathematical rigor of offline formal synthesis with the vigilance of online runtime monitoring to create a verifiable safety net around opaque AI components. We demonstrate its efficacy in an autonomous vehicle perception system, where FAME successfully detected 93.5% of critical safety violations that were otherwise silent. By contextualizing our framework within the ISO 26262 and ISO/PAS 8800 standards, we provide reliability engineers with a practical, certifiable pathway for deploying trustworthy AI. FAME represents a crucial shift from accepting probabilistic performance to enforcing provable safety in next-generation systems.\",\n          \"Retrieval-Augmented Generation (RAG) is a critical technique for grounding Large Language Models (LLMs) in factual evidence, yet evaluating RAG systems in specialized, safety-critical domains remains a significant challenge. Existing evaluation frameworks often rely on heuristic-based metrics that fail to capture domain-specific nuances and other works utilize LLM-as-a-Judge approaches that lack validated alignment with human judgment. This paper introduces RAGalyst, an automated, human-aligned agentic framework designed for the rigorous evaluation of domain-specific RAG systems. RAGalyst features an agentic pipeline that generates high-quality, synthetic question-answering (QA) datasets from source documents, incorporating an agentic filtering step to ensure data fidelity. The framework refines two key LLM-as-a-Judge metrics-Answer Correctness and Answerability-using prompt optimization to achieve a strong correlation with human annotations. Applying this framework to evaluate various RAG components across three distinct domains (military operations, cybersecurity, and bridge engineering), we find that performance is highly context-dependent. No single embedding model, LLM, or hyperparameter configuration proves universally optimal. Additionally, we provide an analysis on the most common low Answer Correctness reasons in RAG. These findings highlight the necessity of a systematic evaluation framework like RAGalyst, which empowers practitioners to uncover domain-specific trade-offs and make informed design choices for building reliable and effective RAG systems. RAGalyst is available on our Github.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 19531,\n        \"samples\": [\n          \"Liang Gong, Tommy, Wang, Sara Chaker, Yanchen Dong\",\n          \"Yu Yan, Sheng Sun, Zhe Wang, Yijun Lin, Zenghao Duan\",\n          \"Shih-Yang Liu, Xin Dong, Ximing Lu, Shizhe Diao, Mingjie Liu\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"date\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 153,\n        \"samples\": [\n          \"2025-09-09\",\n          \"2025-09-07\",\n          \"2025-08-27\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year_month\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"2025-12\",\n          \"2025-11\",\n          \"2025-07\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20000,\n        \"samples\": [\n          \"http://arxiv.org/abs/2509.20386v1\",\n          \"http://arxiv.org/abs/2511.12449v1\",\n          \"http://arxiv.org/abs/2510.01069v1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"categories\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2176,\n        \"samples\": [\n          \"cs.PF, cs.AI, cs.DC, cs.LG\",\n          \"cs.LG, cs.AI, math.RT, stat.ML\",\n          \"cs.AI, cs.CE, cs.CL, cs.CV, stat.ME\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"primary_category\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 115,\n        \"samples\": [\n          \"math.HO\",\n          \"cs.AI\",\n          \"cs.AR\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Create DataFrame\n",
        "df = pd.DataFrame(papers)\n",
        "\n",
        "print(f\"DataFrame shape: {df.shape}\")\n",
        "print(f\"\\nColumns: {list(df.columns)}\")\n",
        "print(f\"\\nDate range: {df['date'].min()} to {df['date'].max()}\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "d8957fb8",
      "metadata": {
        "id": "d8957fb8",
        "outputId": "a7a25412-949b-4109-f382-728fc1c42f4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Statistics:\n",
            "  Total papers: 20000\n",
            "  Unique dates: 153\n",
            "  Date range: 2025-07-03 to 2025-12-02\n",
            "\n",
            "Title length: mean=83, median=83\n",
            "Abstract length: mean=1340, median=1340\n"
          ]
        }
      ],
      "source": [
        "# Basic statistics\n",
        "print(\"Dataset Statistics:\")\n",
        "print(f\"  Total papers: {len(df)}\")\n",
        "print(f\"  Unique dates: {df['date'].nunique()}\")\n",
        "print(f\"  Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "\n",
        "# Text length statistics\n",
        "df['title_len'] = df['title'].str.len()\n",
        "df['abstract_len'] = df['abstract'].str.len()\n",
        "\n",
        "print(f\"\\nTitle length: mean={df['title_len'].mean():.0f}, median={df['title_len'].median():.0f}\")\n",
        "print(f\"Abstract length: mean={df['abstract_len'].mean():.0f}, median={df['abstract_len'].median():.0f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "33536339",
      "metadata": {
        "id": "33536339",
        "outputId": "91e6cd90-df2a-48f1-d5aa-5062e6971068",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Papers per month:\n",
            "year_month\n",
            "2025-07    3156\n",
            "2025-08    3819\n",
            "2025-09    4215\n",
            "2025-10    4821\n",
            "2025-11    3755\n",
            "2025-12     234\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Papers per month\n",
        "papers_per_month = df['year_month'].value_counts().sort_index()\n",
        "print(\"Papers per month:\")\n",
        "print(papers_per_month)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "b160792c",
      "metadata": {
        "id": "b160792c",
        "outputId": "4014e4c5-1844-42dd-e48b-b92e56747d6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample abstracts:\n",
            "================================================================================\n",
            "\n",
            "Title: Fine-Tuned Large Language Models for Logical Translation: Reducing Hallucinations with Lang2Logic\n",
            "Date: 2025-12-02\n",
            "Abstract: Recent advances in natural language processing (NLP), particularly large language models (LLMs), have motivated the automatic translation of natural language statements into formal logic without human intervention. This enables automated reasoning and facilitates debugging, finding loop invariants, ...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Title: Rethinking Generalized BCIs: Benchmarking 340,000+ Unique Algorithmic Configurations for EEG Mental Command Decoding\n",
            "Date: 2025-12-02\n",
            "Abstract: Robust decoding and classification of brain patterns measured with electroencephalography (EEG) remains a major challenge for real-world (i.e. outside scientific lab and medical facilities) brain-computer interface (BCI) applications due to well documented inter- and intra-participant variability. H...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Title: Lumos: Let there be Language Model System Certification\n",
            "Date: 2025-12-02\n",
            "Abstract: We introduce the first principled framework, Lumos, for specifying and formally certifying Language Model System (LMS) behaviors. Lumos is an imperative probabilistic programming DSL over graphs, with constructs to generate independent and identically distributed prompts for LMS. It offers a structu...\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Sample abstracts\n",
        "print(\"Sample abstracts:\")\n",
        "print(\"=\" * 80)\n",
        "for i, row in df.head(3).iterrows():\n",
        "    print(f\"\\nTitle: {row['title']}\")\n",
        "    print(f\"Date: {row['date']}\")\n",
        "    print(f\"Abstract: {row['abstract'][:300]}...\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34a0bddb",
      "metadata": {
        "id": "34a0bddb"
      },
      "source": [
        "## 5. Save Raw Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "b782b907",
      "metadata": {
        "id": "b782b907",
        "outputId": "1fc42212-9ff7-420f-bafd-4d17c8bd0dcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw JSON saved to: /content/drive/MyDrive/BERTopic-arXiv-Analysis/data/raw/arxiv_cs_ai_raw.json\n",
            "Raw CSV saved to: /content/drive/MyDrive/BERTopic-arXiv-Analysis/data/raw/arxiv_cs_ai_raw.csv\n",
            "\n",
            "Total records saved: 20000\n"
          ]
        }
      ],
      "source": [
        "# Save raw data as JSON\n",
        "raw_json_path = f\"{PROJECT_PATH}/data/raw/arxiv_cs_ai_raw.json\"\n",
        "with open(raw_json_path, 'w') as f:\n",
        "    json.dump(papers, f, indent=2)\n",
        "print(f\"Raw JSON saved to: {raw_json_path}\")\n",
        "\n",
        "# Save as CSV (more convenient for pandas)\n",
        "raw_csv_path = f\"{PROJECT_PATH}/data/raw/arxiv_cs_ai_raw.csv\"\n",
        "df.to_csv(raw_csv_path, index=False)\n",
        "print(f\"Raw CSV saved to: {raw_csv_path}\")\n",
        "\n",
        "print(f\"\\nTotal records saved: {len(df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74ac1019",
      "metadata": {
        "id": "74ac1019"
      },
      "source": [
        "## 6. Data Quality Checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "e3fd9151",
      "metadata": {
        "id": "e3fd9151",
        "outputId": "1d819a04-7ad3-448b-b277-9aafeedef8c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values:\n",
            "arxiv_id            0\n",
            "title               0\n",
            "abstract            0\n",
            "authors             0\n",
            "date                0\n",
            "year_month          0\n",
            "url                 0\n",
            "categories          0\n",
            "primary_category    0\n",
            "title_len           0\n",
            "abstract_len        0\n",
            "dtype: int64\n",
            "\n",
            "Duplicate arxiv_ids: 0\n",
            "Short abstracts (<50 chars): 0\n"
          ]
        }
      ],
      "source": [
        "# Check for missing values\n",
        "print(\"Missing values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Check for duplicates\n",
        "n_duplicates = df['arxiv_id'].duplicated().sum()\n",
        "print(f\"\\nDuplicate arxiv_ids: {n_duplicates}\")\n",
        "\n",
        "# Check for empty abstracts\n",
        "empty_abstracts = (df['abstract'].str.len() < 50).sum()\n",
        "print(f\"Short abstracts (<50 chars): {empty_abstracts}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9410da66",
      "metadata": {
        "id": "9410da66"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook has:\n",
        "1. ‚úÖ Fetched arXiv cs.AI papers using the API\n",
        "2. ‚úÖ Created a DataFrame with paper metadata\n",
        "3. ‚úÖ Performed initial data exploration\n",
        "4. ‚úÖ Saved raw data to Google Drive\n",
        "\n",
        "**Next step:** Run `02_preprocessing.ipynb` to clean and prepare the text data."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}